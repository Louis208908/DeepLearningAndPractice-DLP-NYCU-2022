{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from dataloader import read_bci_data\n",
    "from EEGnet import EEGnet\n",
    "from DeepConvNet import DeepConvNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "EPOCH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1, 2, 750) (1080,) (1080, 1, 2, 750) (1080,)\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, datas, labels):\n",
    "        self.datas = datas\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.datas[index], self.labels[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "\n",
    "train_data, train_label, test_data, test_label = read_bci_data()\n",
    "\n",
    "train_data = torch.Tensor(train_data).to(device)\n",
    "train_label = torch.Tensor(train_label).type(torch.LongTensor).to(device)\n",
    "test_data = torch.Tensor(test_data).to(device)\n",
    "test_label = torch.Tensor(test_label).type(torch.LongTensor).to(device)\n",
    "\n",
    "dataset = MyDataset(train_data, train_label)\n",
    "train_loader = data.DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/300, Training loss: 0.9023610949516296, Training accuracy: 0.62890625\n",
      "Test loss: 0.6844357252120972, Test accuracy: 0.67037034034729\n",
      "epoch: 2/300, Training loss: 0.6073467135429382, Training accuracy: 0.70703125\n",
      "Test loss: 0.5770799517631531, Test accuracy: 0.7055555582046509\n",
      "epoch: 3/300, Training loss: 0.5876880884170532, Training accuracy: 0.751953125\n",
      "Test loss: 0.6213428378105164, Test accuracy: 0.7138888835906982\n",
      "epoch: 4/300, Training loss: 0.538352370262146, Training accuracy: 0.75390625\n",
      "Test loss: 0.6393696069717407, Test accuracy: 0.6944444179534912\n",
      "epoch: 5/300, Training loss: 0.551047146320343, Training accuracy: 0.7763671875\n",
      "Test loss: 0.5390657186508179, Test accuracy: 0.7351851463317871\n",
      "epoch: 6/300, Training loss: 0.4823310971260071, Training accuracy: 0.775390625\n",
      "Test loss: 0.5395515561103821, Test accuracy: 0.7314814925193787\n",
      "epoch: 7/300, Training loss: 0.611609697341919, Training accuracy: 0.794921875\n",
      "Test loss: 0.5540807843208313, Test accuracy: 0.7379629611968994\n",
      "epoch: 8/300, Training loss: 0.45837756991386414, Training accuracy: 0.802734375\n",
      "Test loss: 0.5830515027046204, Test accuracy: 0.7222222089767456\n",
      "epoch: 9/300, Training loss: 0.4221988916397095, Training accuracy: 0.8154296875\n",
      "Test loss: 0.6067733764648438, Test accuracy: 0.7277777791023254\n",
      "epoch: 10/300, Training loss: 0.4736316204071045, Training accuracy: 0.8017578125\n",
      "Test loss: 0.5877960324287415, Test accuracy: 0.7138888835906982\n",
      "epoch: 11/300, Training loss: 0.36001232266426086, Training accuracy: 0.8095703125\n",
      "Test loss: 0.5682768821716309, Test accuracy: 0.7314814925193787\n",
      "epoch: 12/300, Training loss: 0.42862579226493835, Training accuracy: 0.8037109375\n",
      "Test loss: 0.5503658652305603, Test accuracy: 0.7583333253860474\n",
      "epoch: 13/300, Training loss: 0.31568631529808044, Training accuracy: 0.8193359375\n",
      "Test loss: 0.5265392065048218, Test accuracy: 0.7657407522201538\n",
      "epoch: 14/300, Training loss: 0.5928177833557129, Training accuracy: 0.8291015625\n",
      "Test loss: 0.6406755447387695, Test accuracy: 0.7055555582046509\n",
      "epoch: 15/300, Training loss: 0.3072243928909302, Training accuracy: 0.8359375\n",
      "Test loss: 0.5701565742492676, Test accuracy: 0.7564814686775208\n",
      "epoch: 16/300, Training loss: 0.3830108046531677, Training accuracy: 0.84375\n",
      "Test loss: 0.5692079067230225, Test accuracy: 0.75\n",
      "epoch: 17/300, Training loss: 0.22233636677265167, Training accuracy: 0.8408203125\n",
      "Test loss: 0.5623299479484558, Test accuracy: 0.7527777552604675\n",
      "epoch: 18/300, Training loss: 0.3604368567466736, Training accuracy: 0.86328125\n",
      "Test loss: 0.5691259503364563, Test accuracy: 0.7555555701255798\n",
      "epoch: 19/300, Training loss: 0.27495425939559937, Training accuracy: 0.8486328125\n",
      "Test loss: 0.5898486375808716, Test accuracy: 0.7527777552604675\n",
      "epoch: 20/300, Training loss: 0.2946396470069885, Training accuracy: 0.8564453125\n",
      "Test loss: 0.5822715759277344, Test accuracy: 0.730555534362793\n",
      "epoch: 21/300, Training loss: 0.4269322454929352, Training accuracy: 0.8447265625\n",
      "Test loss: 0.5712264180183411, Test accuracy: 0.7740740776062012\n",
      "epoch: 22/300, Training loss: 0.33335232734680176, Training accuracy: 0.853515625\n",
      "Test loss: 0.566533088684082, Test accuracy: 0.7592592239379883\n",
      "epoch: 23/300, Training loss: 0.29989761114120483, Training accuracy: 0.8662109375\n",
      "Test loss: 0.5656819343566895, Test accuracy: 0.7694444060325623\n",
      "epoch: 24/300, Training loss: 0.3411104679107666, Training accuracy: 0.87890625\n",
      "Test loss: 0.6195064783096313, Test accuracy: 0.7555555701255798\n",
      "epoch: 25/300, Training loss: 0.38379767537117004, Training accuracy: 0.8701171875\n",
      "Test loss: 0.5923866033554077, Test accuracy: 0.7722222208976746\n",
      "epoch: 26/300, Training loss: 0.2640107274055481, Training accuracy: 0.8984375\n",
      "Test loss: 0.5953909754753113, Test accuracy: 0.7731481194496155\n",
      "epoch: 27/300, Training loss: 0.25540369749069214, Training accuracy: 0.892578125\n",
      "Test loss: 0.6050291061401367, Test accuracy: 0.7768518328666687\n",
      "epoch: 28/300, Training loss: 0.1469690501689911, Training accuracy: 0.9130859375\n",
      "Test loss: 0.6209953427314758, Test accuracy: 0.7759259343147278\n",
      "epoch: 29/300, Training loss: 0.26403966546058655, Training accuracy: 0.8857421875\n",
      "Test loss: 0.6361331343650818, Test accuracy: 0.7657407522201538\n",
      "epoch: 30/300, Training loss: 0.215756356716156, Training accuracy: 0.9052734375\n",
      "Test loss: 0.6061064600944519, Test accuracy: 0.7888888716697693\n",
      "epoch: 31/300, Training loss: 0.22920988500118256, Training accuracy: 0.908203125\n",
      "Test loss: 0.640219509601593, Test accuracy: 0.7648147940635681\n",
      "epoch: 32/300, Training loss: 0.1264183223247528, Training accuracy: 0.9130859375\n",
      "Test loss: 0.6183608770370483, Test accuracy: 0.770370364189148\n",
      "epoch: 33/300, Training loss: 0.1674119532108307, Training accuracy: 0.904296875\n",
      "Test loss: 0.64065021276474, Test accuracy: 0.760185182094574\n",
      "epoch: 34/300, Training loss: 0.19366568326950073, Training accuracy: 0.916015625\n",
      "Test loss: 0.593604564666748, Test accuracy: 0.7694444060325623\n",
      "epoch: 35/300, Training loss: 0.21974413096904755, Training accuracy: 0.9189453125\n",
      "Test loss: 0.589922308921814, Test accuracy: 0.7759259343147278\n",
      "epoch: 36/300, Training loss: 0.1407231241464615, Training accuracy: 0.9345703125\n",
      "Test loss: 0.6415215134620667, Test accuracy: 0.7787036895751953\n",
      "epoch: 37/300, Training loss: 0.2033853679895401, Training accuracy: 0.9267578125\n",
      "Test loss: 0.6387369632720947, Test accuracy: 0.7648147940635681\n",
      "epoch: 38/300, Training loss: 0.30262500047683716, Training accuracy: 0.9091796875\n",
      "Test loss: 0.6259083151817322, Test accuracy: 0.7648147940635681\n",
      "epoch: 39/300, Training loss: 0.2051256000995636, Training accuracy: 0.919921875\n",
      "Test loss: 0.6145361065864563, Test accuracy: 0.7749999761581421\n",
      "epoch: 40/300, Training loss: 0.17648562788963318, Training accuracy: 0.939453125\n",
      "Test loss: 0.6362878084182739, Test accuracy: 0.7592592239379883\n",
      "epoch: 41/300, Training loss: 0.12033351510763168, Training accuracy: 0.9248046875\n",
      "Test loss: 0.6136490702629089, Test accuracy: 0.7805555462837219\n",
      "epoch: 42/300, Training loss: 0.26578399538993835, Training accuracy: 0.93359375\n",
      "Test loss: 0.6578763127326965, Test accuracy: 0.7740740776062012\n",
      "epoch: 43/300, Training loss: 0.26528945565223694, Training accuracy: 0.9365234375\n",
      "Test loss: 0.7429946064949036, Test accuracy: 0.7685185074806213\n",
      "epoch: 44/300, Training loss: 0.18362754583358765, Training accuracy: 0.93359375\n",
      "Test loss: 0.6789350509643555, Test accuracy: 0.7824074029922485\n",
      "epoch: 45/300, Training loss: 0.1704738438129425, Training accuracy: 0.939453125\n",
      "Test loss: 0.6719052195549011, Test accuracy: 0.7620370388031006\n",
      "epoch: 46/300, Training loss: 0.2878449857234955, Training accuracy: 0.9384765625\n",
      "Test loss: 0.7109901309013367, Test accuracy: 0.7638888955116272\n",
      "epoch: 47/300, Training loss: 0.15264523029327393, Training accuracy: 0.953125\n",
      "Test loss: 0.6790469884872437, Test accuracy: 0.7694444060325623\n",
      "epoch: 48/300, Training loss: 0.16564244031906128, Training accuracy: 0.9306640625\n",
      "Test loss: 0.7033197283744812, Test accuracy: 0.7796295881271362\n",
      "epoch: 49/300, Training loss: 0.19898471236228943, Training accuracy: 0.9501953125\n",
      "Test loss: 0.7143504619598389, Test accuracy: 0.7611110806465149\n",
      "epoch: 50/300, Training loss: 0.10750062018632889, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6962738037109375, Test accuracy: 0.7796295881271362\n",
      "epoch: 51/300, Training loss: 0.15522775053977966, Training accuracy: 0.9482421875\n",
      "Test loss: 0.6961311101913452, Test accuracy: 0.7749999761581421\n",
      "epoch: 52/300, Training loss: 0.11484818905591965, Training accuracy: 0.953125\n",
      "Test loss: 0.6764742732048035, Test accuracy: 0.7925925850868225\n",
      "epoch: 53/300, Training loss: 0.12278041988611221, Training accuracy: 0.943359375\n",
      "Test loss: 0.6767137050628662, Test accuracy: 0.7805555462837219\n",
      "epoch: 54/300, Training loss: 0.12784956395626068, Training accuracy: 0.953125\n",
      "Test loss: 0.6690252423286438, Test accuracy: 0.7870370149612427\n",
      "epoch: 55/300, Training loss: 0.12607944011688232, Training accuracy: 0.9384765625\n",
      "Test loss: 0.7245826721191406, Test accuracy: 0.7740740776062012\n",
      "epoch: 56/300, Training loss: 0.11875186115503311, Training accuracy: 0.9609375\n",
      "Test loss: 0.6999678611755371, Test accuracy: 0.7712962627410889\n",
      "epoch: 57/300, Training loss: 0.08891206979751587, Training accuracy: 0.9560546875\n",
      "Test loss: 0.6797120571136475, Test accuracy: 0.7824074029922485\n",
      "epoch: 58/300, Training loss: 0.16895891726016998, Training accuracy: 0.947265625\n",
      "Test loss: 0.7428426742553711, Test accuracy: 0.760185182094574\n",
      "epoch: 59/300, Training loss: 0.15730272233486176, Training accuracy: 0.953125\n",
      "Test loss: 0.7122437357902527, Test accuracy: 0.7759259343147278\n",
      "epoch: 60/300, Training loss: 0.13991205394268036, Training accuracy: 0.9541015625\n",
      "Test loss: 0.7628089189529419, Test accuracy: 0.7787036895751953\n",
      "epoch: 61/300, Training loss: 0.08720002323389053, Training accuracy: 0.9521484375\n",
      "Test loss: 0.7266438603401184, Test accuracy: 0.7814814448356628\n",
      "epoch: 62/300, Training loss: 0.11906999349594116, Training accuracy: 0.955078125\n",
      "Test loss: 0.6752716898918152, Test accuracy: 0.7861111164093018\n",
      "epoch: 63/300, Training loss: 0.0722353383898735, Training accuracy: 0.9501953125\n",
      "Test loss: 0.7063759565353394, Test accuracy: 0.7740740776062012\n",
      "epoch: 64/300, Training loss: 0.21578402817249298, Training accuracy: 0.9501953125\n",
      "Test loss: 0.8000016808509827, Test accuracy: 0.7509258985519409\n",
      "epoch: 65/300, Training loss: 0.06915821880102158, Training accuracy: 0.9580078125\n",
      "Test loss: 0.749748945236206, Test accuracy: 0.7675925493240356\n",
      "epoch: 66/300, Training loss: 0.10702540725469589, Training accuracy: 0.962890625\n",
      "Test loss: 0.7773746252059937, Test accuracy: 0.7787036895751953\n",
      "epoch: 67/300, Training loss: 0.15051306784152985, Training accuracy: 0.9599609375\n",
      "Test loss: 0.7556757926940918, Test accuracy: 0.7814814448356628\n",
      "epoch: 68/300, Training loss: 0.13331981003284454, Training accuracy: 0.9599609375\n",
      "Test loss: 0.7468549013137817, Test accuracy: 0.7749999761581421\n",
      "epoch: 69/300, Training loss: 0.13910257816314697, Training accuracy: 0.966796875\n",
      "Test loss: 0.7630501985549927, Test accuracy: 0.7787036895751953\n",
      "epoch: 70/300, Training loss: 0.10817720741033554, Training accuracy: 0.962890625\n",
      "Test loss: 0.7603689432144165, Test accuracy: 0.7712962627410889\n",
      "epoch: 71/300, Training loss: 0.12858715653419495, Training accuracy: 0.96875\n",
      "Test loss: 0.7072656154632568, Test accuracy: 0.7796295881271362\n",
      "epoch: 72/300, Training loss: 0.08355910331010818, Training accuracy: 0.9658203125\n",
      "Test loss: 0.7454054355621338, Test accuracy: 0.7731481194496155\n",
      "epoch: 73/300, Training loss: 0.06697919219732285, Training accuracy: 0.951171875\n",
      "Test loss: 0.7517317533493042, Test accuracy: 0.7796295881271362\n",
      "epoch: 74/300, Training loss: 0.07910902798175812, Training accuracy: 0.9716796875\n",
      "Test loss: 0.7677274346351624, Test accuracy: 0.7787036895751953\n",
      "epoch: 75/300, Training loss: 0.09837333858013153, Training accuracy: 0.96484375\n",
      "Test loss: 0.7600252628326416, Test accuracy: 0.7805555462837219\n",
      "epoch: 76/300, Training loss: 0.07112129777669907, Training accuracy: 0.9677734375\n",
      "Test loss: 0.7131350040435791, Test accuracy: 0.7749999761581421\n",
      "epoch: 77/300, Training loss: 0.09025344997644424, Training accuracy: 0.9658203125\n",
      "Test loss: 0.7312288284301758, Test accuracy: 0.7907407283782959\n",
      "epoch: 78/300, Training loss: 0.04805009812116623, Training accuracy: 0.9638671875\n",
      "Test loss: 0.7305822372436523, Test accuracy: 0.7842592597007751\n",
      "epoch: 79/300, Training loss: 0.09799730032682419, Training accuracy: 0.9755859375\n",
      "Test loss: 0.7726489305496216, Test accuracy: 0.7824074029922485\n",
      "epoch: 80/300, Training loss: 0.07684287428855896, Training accuracy: 0.962890625\n",
      "Test loss: 0.7839125394821167, Test accuracy: 0.7851851582527161\n",
      "epoch: 81/300, Training loss: 0.05476640164852142, Training accuracy: 0.9677734375\n",
      "Test loss: 0.7941866517066956, Test accuracy: 0.7768518328666687\n",
      "epoch: 82/300, Training loss: 0.0902823954820633, Training accuracy: 0.966796875\n",
      "Test loss: 0.758592963218689, Test accuracy: 0.7907407283782959\n",
      "epoch: 83/300, Training loss: 0.11737960577011108, Training accuracy: 0.9541015625\n",
      "Test loss: 0.7726868987083435, Test accuracy: 0.7787036895751953\n",
      "epoch: 84/300, Training loss: 0.11308358609676361, Training accuracy: 0.9697265625\n",
      "Test loss: 0.732268750667572, Test accuracy: 0.7824074029922485\n",
      "epoch: 85/300, Training loss: 0.09644602984189987, Training accuracy: 0.955078125\n",
      "Test loss: 0.7195119261741638, Test accuracy: 0.7824074029922485\n",
      "epoch: 86/300, Training loss: 0.11078108847141266, Training accuracy: 0.96875\n",
      "Test loss: 0.7782850861549377, Test accuracy: 0.7888888716697693\n",
      "epoch: 87/300, Training loss: 0.20175591111183167, Training accuracy: 0.9619140625\n",
      "Test loss: 0.7897887229919434, Test accuracy: 0.7851851582527161\n",
      "epoch: 88/300, Training loss: 0.08513963967561722, Training accuracy: 0.9765625\n",
      "Test loss: 0.786230206489563, Test accuracy: 0.7814814448356628\n",
      "epoch: 89/300, Training loss: 0.05875270813703537, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7968046069145203, Test accuracy: 0.7694444060325623\n",
      "epoch: 90/300, Training loss: 0.1327635794878006, Training accuracy: 0.9541015625\n",
      "Test loss: 0.7270861268043518, Test accuracy: 0.7981481552124023\n",
      "epoch: 91/300, Training loss: 0.0459664948284626, Training accuracy: 0.9755859375\n",
      "Test loss: 0.728760838508606, Test accuracy: 0.7851851582527161\n",
      "epoch: 92/300, Training loss: 0.047512974590063095, Training accuracy: 0.966796875\n",
      "Test loss: 0.8391754031181335, Test accuracy: 0.7879629731178284\n",
      "epoch: 93/300, Training loss: 0.10100079327821732, Training accuracy: 0.9677734375\n",
      "Test loss: 0.7393773794174194, Test accuracy: 0.7842592597007751\n",
      "epoch: 94/300, Training loss: 0.08889063447713852, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7579581141471863, Test accuracy: 0.7712962627410889\n",
      "epoch: 95/300, Training loss: 0.05060559883713722, Training accuracy: 0.97265625\n",
      "Test loss: 0.7820795178413391, Test accuracy: 0.7740740776062012\n",
      "epoch: 96/300, Training loss: 0.1126909926533699, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7327895164489746, Test accuracy: 0.7935184836387634\n",
      "epoch: 97/300, Training loss: 0.08937215059995651, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7794962525367737, Test accuracy: 0.7814814448356628\n",
      "epoch: 98/300, Training loss: 0.09556545317173004, Training accuracy: 0.9755859375\n",
      "Test loss: 0.7854387164115906, Test accuracy: 0.7777777910232544\n",
      "epoch: 99/300, Training loss: 0.13466192781925201, Training accuracy: 0.966796875\n",
      "Test loss: 0.7684021592140198, Test accuracy: 0.7824074029922485\n",
      "epoch: 100/300, Training loss: 0.0889342874288559, Training accuracy: 0.9677734375\n",
      "Test loss: 0.7232930660247803, Test accuracy: 0.7833333015441895\n",
      "epoch: 101/300, Training loss: 0.0800476148724556, Training accuracy: 0.962890625\n",
      "Test loss: 0.7723103761672974, Test accuracy: 0.7842592597007751\n",
      "epoch: 102/300, Training loss: 0.10753991454839706, Training accuracy: 0.96875\n",
      "Test loss: 0.7447211146354675, Test accuracy: 0.7888888716697693\n",
      "epoch: 103/300, Training loss: 0.10209875553846359, Training accuracy: 0.9677734375\n",
      "Test loss: 0.7427961826324463, Test accuracy: 0.770370364189148\n",
      "epoch: 104/300, Training loss: 0.13211128115653992, Training accuracy: 0.962890625\n",
      "Test loss: 0.7628028988838196, Test accuracy: 0.7824074029922485\n",
      "epoch: 105/300, Training loss: 0.0978192612528801, Training accuracy: 0.9765625\n",
      "Test loss: 0.7831310033798218, Test accuracy: 0.7675925493240356\n",
      "epoch: 106/300, Training loss: 0.11543920636177063, Training accuracy: 0.9765625\n",
      "Test loss: 0.7738480567932129, Test accuracy: 0.7888888716697693\n",
      "epoch: 107/300, Training loss: 0.10121244937181473, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7348307967185974, Test accuracy: 0.789814829826355\n",
      "epoch: 108/300, Training loss: 0.09835126996040344, Training accuracy: 0.97265625\n",
      "Test loss: 0.8095481395721436, Test accuracy: 0.7740740776062012\n",
      "epoch: 109/300, Training loss: 0.12536105513572693, Training accuracy: 0.9716796875\n",
      "Test loss: 0.7362369298934937, Test accuracy: 0.7851851582527161\n",
      "epoch: 110/300, Training loss: 0.07601682841777802, Training accuracy: 0.9814453125\n",
      "Test loss: 0.7877130508422852, Test accuracy: 0.7833333015441895\n",
      "epoch: 111/300, Training loss: 0.10295025259256363, Training accuracy: 0.9755859375\n",
      "Test loss: 0.796398937702179, Test accuracy: 0.7675925493240356\n",
      "epoch: 112/300, Training loss: 0.09496349841356277, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8022706508636475, Test accuracy: 0.7629629373550415\n",
      "epoch: 113/300, Training loss: 0.22727079689502716, Training accuracy: 0.96875\n",
      "Test loss: 0.7662224769592285, Test accuracy: 0.7879629731178284\n",
      "epoch: 114/300, Training loss: 0.0326964296400547, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7615866661071777, Test accuracy: 0.7907407283782959\n",
      "epoch: 115/300, Training loss: 0.03651321306824684, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7984311580657959, Test accuracy: 0.7675925493240356\n",
      "epoch: 116/300, Training loss: 0.16899120807647705, Training accuracy: 0.96484375\n",
      "Test loss: 0.8184500932693481, Test accuracy: 0.7814814448356628\n",
      "epoch: 117/300, Training loss: 0.07219048589468002, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7824656367301941, Test accuracy: 0.7796295881271362\n",
      "epoch: 118/300, Training loss: 0.11074704676866531, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7530948519706726, Test accuracy: 0.7824074029922485\n",
      "epoch: 119/300, Training loss: 0.03815354406833649, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8052182197570801, Test accuracy: 0.7657407522201538\n",
      "epoch: 120/300, Training loss: 0.06189068779349327, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8051304817199707, Test accuracy: 0.7888888716697693\n",
      "epoch: 121/300, Training loss: 0.09203525632619858, Training accuracy: 0.974609375\n",
      "Test loss: 0.7309929132461548, Test accuracy: 0.7861111164093018\n",
      "epoch: 122/300, Training loss: 0.11921621114015579, Training accuracy: 0.970703125\n",
      "Test loss: 0.7567512392997742, Test accuracy: 0.7851851582527161\n",
      "epoch: 123/300, Training loss: 0.06347833573818207, Training accuracy: 0.9716796875\n",
      "Test loss: 0.7697193026542664, Test accuracy: 0.7962962985038757\n",
      "epoch: 124/300, Training loss: 0.08323047310113907, Training accuracy: 0.978515625\n",
      "Test loss: 0.8042082190513611, Test accuracy: 0.7833333015441895\n",
      "epoch: 125/300, Training loss: 0.07450088113546371, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8105129599571228, Test accuracy: 0.7675925493240356\n",
      "epoch: 126/300, Training loss: 0.07616636157035828, Training accuracy: 0.9765625\n",
      "Test loss: 0.766421914100647, Test accuracy: 0.7768518328666687\n",
      "epoch: 127/300, Training loss: 0.03251074627041817, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8018513917922974, Test accuracy: 0.7796295881271362\n",
      "epoch: 128/300, Training loss: 0.0604066401720047, Training accuracy: 0.96875\n",
      "Test loss: 0.7461576461791992, Test accuracy: 0.7814814448356628\n",
      "epoch: 129/300, Training loss: 0.12436338514089584, Training accuracy: 0.970703125\n",
      "Test loss: 0.7688413858413696, Test accuracy: 0.7916666269302368\n",
      "epoch: 130/300, Training loss: 0.07766668498516083, Training accuracy: 0.9765625\n",
      "Test loss: 0.7895358204841614, Test accuracy: 0.7851851582527161\n",
      "epoch: 131/300, Training loss: 0.06949836760759354, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7911333441734314, Test accuracy: 0.7814814448356628\n",
      "epoch: 132/300, Training loss: 0.05908150225877762, Training accuracy: 0.9814453125\n",
      "Test loss: 0.761296272277832, Test accuracy: 0.7972221970558167\n",
      "epoch: 133/300, Training loss: 0.09013011306524277, Training accuracy: 0.9765625\n",
      "Test loss: 0.783089816570282, Test accuracy: 0.7916666269302368\n",
      "epoch: 134/300, Training loss: 0.106352798640728, Training accuracy: 0.966796875\n",
      "Test loss: 0.7917647361755371, Test accuracy: 0.7814814448356628\n",
      "epoch: 135/300, Training loss: 0.09063532203435898, Training accuracy: 0.9677734375\n",
      "Test loss: 0.7541698217391968, Test accuracy: 0.7842592597007751\n",
      "epoch: 136/300, Training loss: 0.06003726273775101, Training accuracy: 0.97265625\n",
      "Test loss: 0.770830512046814, Test accuracy: 0.7787036895751953\n",
      "epoch: 137/300, Training loss: 0.05636328458786011, Training accuracy: 0.970703125\n",
      "Test loss: 0.8081679344177246, Test accuracy: 0.7731481194496155\n",
      "epoch: 138/300, Training loss: 0.054315630346536636, Training accuracy: 0.970703125\n",
      "Test loss: 0.8374775052070618, Test accuracy: 0.7731481194496155\n",
      "epoch: 139/300, Training loss: 0.05899081006646156, Training accuracy: 0.970703125\n",
      "Test loss: 0.7672789692878723, Test accuracy: 0.7879629731178284\n",
      "epoch: 140/300, Training loss: 0.04188485071063042, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8173351883888245, Test accuracy: 0.7814814448356628\n",
      "epoch: 141/300, Training loss: 0.040018193423748016, Training accuracy: 0.9765625\n",
      "Test loss: 0.8047361969947815, Test accuracy: 0.7888888716697693\n",
      "epoch: 142/300, Training loss: 0.06383977085351944, Training accuracy: 0.984375\n",
      "Test loss: 0.8112227916717529, Test accuracy: 0.7749999761581421\n",
      "epoch: 143/300, Training loss: 0.0928746908903122, Training accuracy: 0.9677734375\n",
      "Test loss: 0.8223584890365601, Test accuracy: 0.7685185074806213\n",
      "epoch: 144/300, Training loss: 0.05722573772072792, Training accuracy: 0.982421875\n",
      "Test loss: 0.8128848671913147, Test accuracy: 0.7796295881271362\n",
      "epoch: 145/300, Training loss: 0.09140457957983017, Training accuracy: 0.984375\n",
      "Test loss: 0.8105032444000244, Test accuracy: 0.7805555462837219\n",
      "epoch: 146/300, Training loss: 0.05042693391442299, Training accuracy: 0.9658203125\n",
      "Test loss: 0.8058363795280457, Test accuracy: 0.7814814448356628\n",
      "epoch: 147/300, Training loss: 0.08022205531597137, Training accuracy: 0.970703125\n",
      "Test loss: 0.8471001386642456, Test accuracy: 0.7833333015441895\n",
      "epoch: 148/300, Training loss: 0.07116281986236572, Training accuracy: 0.978515625\n",
      "Test loss: 0.7964589595794678, Test accuracy: 0.7842592597007751\n",
      "epoch: 149/300, Training loss: 0.1549268215894699, Training accuracy: 0.97265625\n",
      "Test loss: 0.8282158374786377, Test accuracy: 0.7768518328666687\n",
      "epoch: 150/300, Training loss: 0.21271708607673645, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8256121873855591, Test accuracy: 0.789814829826355\n",
      "epoch: 151/300, Training loss: 0.11265373975038528, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7877870202064514, Test accuracy: 0.7833333015441895\n",
      "epoch: 152/300, Training loss: 0.06565620005130768, Training accuracy: 0.97265625\n",
      "Test loss: 0.7791104316711426, Test accuracy: 0.7824074029922485\n",
      "epoch: 153/300, Training loss: 0.07665020227432251, Training accuracy: 0.97265625\n",
      "Test loss: 0.7533610463142395, Test accuracy: 0.789814829826355\n",
      "epoch: 154/300, Training loss: 0.14364632964134216, Training accuracy: 0.9677734375\n",
      "Test loss: 0.7641264796257019, Test accuracy: 0.7907407283782959\n",
      "epoch: 155/300, Training loss: 0.08083534240722656, Training accuracy: 0.9697265625\n",
      "Test loss: 0.8079375624656677, Test accuracy: 0.7712962627410889\n",
      "epoch: 156/300, Training loss: 0.03645925968885422, Training accuracy: 0.978515625\n",
      "Test loss: 0.8022245764732361, Test accuracy: 0.7805555462837219\n",
      "epoch: 157/300, Training loss: 0.06516725569963455, Training accuracy: 0.9697265625\n",
      "Test loss: 0.8720276355743408, Test accuracy: 0.7749999761581421\n",
      "epoch: 158/300, Training loss: 0.0825803205370903, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8172216415405273, Test accuracy: 0.7787036895751953\n",
      "epoch: 159/300, Training loss: 0.11197911202907562, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8142471313476562, Test accuracy: 0.770370364189148\n",
      "epoch: 160/300, Training loss: 0.01836813986301422, Training accuracy: 0.9765625\n",
      "Test loss: 0.7471333146095276, Test accuracy: 0.8018518090248108\n",
      "epoch: 161/300, Training loss: 0.030259618535637856, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8291605710983276, Test accuracy: 0.7879629731178284\n",
      "epoch: 162/300, Training loss: 0.05835190415382385, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8185216188430786, Test accuracy: 0.7851851582527161\n",
      "epoch: 163/300, Training loss: 0.057704951614141464, Training accuracy: 0.9697265625\n",
      "Test loss: 0.8219227194786072, Test accuracy: 0.7851851582527161\n",
      "epoch: 164/300, Training loss: 0.02398577891290188, Training accuracy: 0.9765625\n",
      "Test loss: 0.7891806960105896, Test accuracy: 0.7833333015441895\n",
      "epoch: 165/300, Training loss: 0.11708125472068787, Training accuracy: 0.966796875\n",
      "Test loss: 0.8281147480010986, Test accuracy: 0.7805555462837219\n",
      "epoch: 166/300, Training loss: 0.0773271918296814, Training accuracy: 0.970703125\n",
      "Test loss: 0.8384820818901062, Test accuracy: 0.7759259343147278\n",
      "epoch: 167/300, Training loss: 0.05477016419172287, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8213432431221008, Test accuracy: 0.7787036895751953\n",
      "epoch: 168/300, Training loss: 0.06812798231840134, Training accuracy: 0.982421875\n",
      "Test loss: 0.8058483600616455, Test accuracy: 0.7824074029922485\n",
      "epoch: 169/300, Training loss: 0.04123781993985176, Training accuracy: 0.9853515625\n",
      "Test loss: 0.8287757635116577, Test accuracy: 0.7768518328666687\n",
      "epoch: 170/300, Training loss: 0.08210122585296631, Training accuracy: 0.96875\n",
      "Test loss: 0.7642390727996826, Test accuracy: 0.7916666269302368\n",
      "epoch: 171/300, Training loss: 0.08478578925132751, Training accuracy: 0.982421875\n",
      "Test loss: 0.7711514234542847, Test accuracy: 0.7944444417953491\n",
      "epoch: 172/300, Training loss: 0.07641217112541199, Training accuracy: 0.97265625\n",
      "Test loss: 0.8373197913169861, Test accuracy: 0.7861111164093018\n",
      "epoch: 173/300, Training loss: 0.04224929213523865, Training accuracy: 0.9765625\n",
      "Test loss: 0.8156717419624329, Test accuracy: 0.7879629731178284\n",
      "epoch: 174/300, Training loss: 0.04202774912118912, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7759115695953369, Test accuracy: 0.7888888716697693\n",
      "epoch: 175/300, Training loss: 0.1384575366973877, Training accuracy: 0.97265625\n",
      "Test loss: 0.8392817378044128, Test accuracy: 0.7768518328666687\n",
      "epoch: 176/300, Training loss: 0.04569843038916588, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8483955264091492, Test accuracy: 0.7694444060325623\n",
      "epoch: 177/300, Training loss: 0.0831049457192421, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7999985218048096, Test accuracy: 0.7777777910232544\n",
      "epoch: 178/300, Training loss: 0.04636204615235329, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7926937937736511, Test accuracy: 0.7879629731178284\n",
      "epoch: 179/300, Training loss: 0.08520635217428207, Training accuracy: 0.9814453125\n",
      "Test loss: 0.7976254224777222, Test accuracy: 0.7916666269302368\n",
      "epoch: 180/300, Training loss: 0.06147284805774689, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8042019605636597, Test accuracy: 0.789814829826355\n",
      "epoch: 181/300, Training loss: 0.03688642755150795, Training accuracy: 0.974609375\n",
      "Test loss: 0.8236895799636841, Test accuracy: 0.7759259343147278\n",
      "epoch: 182/300, Training loss: 0.08750344812870026, Training accuracy: 0.9833984375\n",
      "Test loss: 0.8073859810829163, Test accuracy: 0.7842592597007751\n",
      "epoch: 183/300, Training loss: 0.3469413220882416, Training accuracy: 0.9775390625\n",
      "Test loss: 0.766054630279541, Test accuracy: 0.7768518328666687\n",
      "epoch: 184/300, Training loss: 0.0626467764377594, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7999575138092041, Test accuracy: 0.7870370149612427\n",
      "epoch: 185/300, Training loss: 0.052162785083055496, Training accuracy: 0.978515625\n",
      "Test loss: 0.8062719106674194, Test accuracy: 0.7842592597007751\n",
      "epoch: 186/300, Training loss: 0.04468309506773949, Training accuracy: 0.978515625\n",
      "Test loss: 0.7823741436004639, Test accuracy: 0.7842592597007751\n",
      "epoch: 187/300, Training loss: 0.0712372213602066, Training accuracy: 0.978515625\n",
      "Test loss: 0.8682982325553894, Test accuracy: 0.7740740776062012\n",
      "epoch: 188/300, Training loss: 0.10924740135669708, Training accuracy: 0.9833984375\n",
      "Test loss: 0.7409104704856873, Test accuracy: 0.7879629731178284\n",
      "epoch: 189/300, Training loss: 0.06026897579431534, Training accuracy: 0.970703125\n",
      "Test loss: 0.7924160361289978, Test accuracy: 0.7888888716697693\n",
      "epoch: 190/300, Training loss: 0.14517465233802795, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7637024521827698, Test accuracy: 0.7805555462837219\n",
      "epoch: 191/300, Training loss: 0.03737345337867737, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8489712476730347, Test accuracy: 0.7805555462837219\n",
      "epoch: 192/300, Training loss: 0.05958590283989906, Training accuracy: 0.9677734375\n",
      "Test loss: 0.8267347812652588, Test accuracy: 0.7981481552124023\n",
      "epoch: 193/300, Training loss: 0.1269051730632782, Training accuracy: 0.970703125\n",
      "Test loss: 0.7856872081756592, Test accuracy: 0.8037036657333374\n",
      "epoch: 194/300, Training loss: 0.09150567650794983, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7739290595054626, Test accuracy: 0.7907407283782959\n",
      "epoch: 195/300, Training loss: 0.03820297494530678, Training accuracy: 0.982421875\n",
      "Test loss: 0.7880439162254333, Test accuracy: 0.7805555462837219\n",
      "epoch: 196/300, Training loss: 0.0879625529050827, Training accuracy: 0.98046875\n",
      "Test loss: 0.7412402629852295, Test accuracy: 0.7925925850868225\n",
      "epoch: 197/300, Training loss: 0.03497437387704849, Training accuracy: 0.9775390625\n",
      "Test loss: 0.871368408203125, Test accuracy: 0.7666666507720947\n",
      "epoch: 198/300, Training loss: 0.06526446342468262, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7846848368644714, Test accuracy: 0.7870370149612427\n",
      "epoch: 199/300, Training loss: 0.10809648036956787, Training accuracy: 0.9716796875\n",
      "Test loss: 0.7995081543922424, Test accuracy: 0.7870370149612427\n",
      "epoch: 200/300, Training loss: 0.0749964565038681, Training accuracy: 0.98046875\n",
      "Test loss: 0.7439680099487305, Test accuracy: 0.7962962985038757\n",
      "epoch: 201/300, Training loss: 0.13410581648349762, Training accuracy: 0.9765625\n",
      "Test loss: 0.8066356778144836, Test accuracy: 0.7824074029922485\n",
      "epoch: 202/300, Training loss: 0.012726305983960629, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7727810740470886, Test accuracy: 0.7712962627410889\n",
      "epoch: 203/300, Training loss: 0.06122562289237976, Training accuracy: 0.9814453125\n",
      "Test loss: 0.7559143304824829, Test accuracy: 0.7916666269302368\n",
      "epoch: 204/300, Training loss: 0.05945475772023201, Training accuracy: 0.96875\n",
      "Test loss: 0.8071866631507874, Test accuracy: 0.7888888716697693\n",
      "epoch: 205/300, Training loss: 0.07718870788812637, Training accuracy: 0.978515625\n",
      "Test loss: 0.8356462717056274, Test accuracy: 0.7787036895751953\n",
      "epoch: 206/300, Training loss: 0.0696549341082573, Training accuracy: 0.96484375\n",
      "Test loss: 0.7910173535346985, Test accuracy: 0.7870370149612427\n",
      "epoch: 207/300, Training loss: 0.08167901635169983, Training accuracy: 0.9677734375\n",
      "Test loss: 0.7892295718193054, Test accuracy: 0.7814814448356628\n",
      "epoch: 208/300, Training loss: 0.09322406351566315, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7770118117332458, Test accuracy: 0.79537034034729\n",
      "epoch: 209/300, Training loss: 0.05788344517350197, Training accuracy: 0.966796875\n",
      "Test loss: 0.8102362155914307, Test accuracy: 0.7833333015441895\n",
      "epoch: 210/300, Training loss: 0.06273574382066727, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8036442399024963, Test accuracy: 0.7675925493240356\n",
      "epoch: 211/300, Training loss: 0.06371215730905533, Training accuracy: 0.9853515625\n",
      "Test loss: 0.8070361614227295, Test accuracy: 0.7740740776062012\n",
      "epoch: 212/300, Training loss: 0.0805373415350914, Training accuracy: 0.982421875\n",
      "Test loss: 0.8129322528839111, Test accuracy: 0.7870370149612427\n",
      "epoch: 213/300, Training loss: 0.08518334478139877, Training accuracy: 0.970703125\n",
      "Test loss: 0.8676920533180237, Test accuracy: 0.7851851582527161\n",
      "epoch: 214/300, Training loss: 0.11775002628564835, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7427776455879211, Test accuracy: 0.7879629731178284\n",
      "epoch: 215/300, Training loss: 0.1241426020860672, Training accuracy: 0.974609375\n",
      "Test loss: 0.7757297158241272, Test accuracy: 0.7907407283782959\n",
      "epoch: 216/300, Training loss: 0.1286618560552597, Training accuracy: 0.97265625\n",
      "Test loss: 0.8332591652870178, Test accuracy: 0.7824074029922485\n",
      "epoch: 217/300, Training loss: 0.08735153079032898, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8460110425949097, Test accuracy: 0.7712962627410889\n",
      "epoch: 218/300, Training loss: 0.017682628706097603, Training accuracy: 0.9833984375\n",
      "Test loss: 0.7866163849830627, Test accuracy: 0.7731481194496155\n",
      "epoch: 219/300, Training loss: 0.034392669796943665, Training accuracy: 0.98046875\n",
      "Test loss: 0.8107492923736572, Test accuracy: 0.7842592597007751\n",
      "epoch: 220/300, Training loss: 0.13071471452713013, Training accuracy: 0.970703125\n",
      "Test loss: 0.8356876373291016, Test accuracy: 0.7925925850868225\n",
      "epoch: 221/300, Training loss: 0.10513141751289368, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8329949378967285, Test accuracy: 0.7861111164093018\n",
      "epoch: 222/300, Training loss: 0.0982661172747612, Training accuracy: 0.9765625\n",
      "Test loss: 0.7970513105392456, Test accuracy: 0.7879629731178284\n",
      "epoch: 223/300, Training loss: 0.06588847935199738, Training accuracy: 0.9794921875\n",
      "Test loss: 0.824098527431488, Test accuracy: 0.7787036895751953\n",
      "epoch: 224/300, Training loss: 0.06608019024133682, Training accuracy: 0.978515625\n",
      "Test loss: 0.7958914041519165, Test accuracy: 0.7805555462837219\n",
      "epoch: 225/300, Training loss: 0.06654837727546692, Training accuracy: 0.9765625\n",
      "Test loss: 0.8142679929733276, Test accuracy: 0.7824074029922485\n",
      "epoch: 226/300, Training loss: 0.0376371406018734, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8492011427879333, Test accuracy: 0.7685185074806213\n",
      "epoch: 227/300, Training loss: 0.024997977539896965, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8620840907096863, Test accuracy: 0.7824074029922485\n",
      "epoch: 228/300, Training loss: 0.0601704902946949, Training accuracy: 0.978515625\n",
      "Test loss: 0.8500792980194092, Test accuracy: 0.7611110806465149\n",
      "epoch: 229/300, Training loss: 0.05991305783390999, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7943978905677795, Test accuracy: 0.7870370149612427\n",
      "epoch: 230/300, Training loss: 0.07967516779899597, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7659806609153748, Test accuracy: 0.7870370149612427\n",
      "epoch: 231/300, Training loss: 0.06621117889881134, Training accuracy: 0.978515625\n",
      "Test loss: 0.804706335067749, Test accuracy: 0.7925925850868225\n",
      "epoch: 232/300, Training loss: 0.138325497508049, Training accuracy: 0.982421875\n",
      "Test loss: 0.8134088516235352, Test accuracy: 0.7861111164093018\n",
      "epoch: 233/300, Training loss: 0.04164617881178856, Training accuracy: 0.9755859375\n",
      "Test loss: 0.817905604839325, Test accuracy: 0.7907407283782959\n",
      "epoch: 234/300, Training loss: 0.04489092901349068, Training accuracy: 0.9658203125\n",
      "Test loss: 0.7939143776893616, Test accuracy: 0.7824074029922485\n",
      "epoch: 235/300, Training loss: 0.060028478503227234, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8469744920730591, Test accuracy: 0.7749999761581421\n",
      "epoch: 236/300, Training loss: 0.11393124610185623, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7531063556671143, Test accuracy: 0.7861111164093018\n",
      "epoch: 237/300, Training loss: 0.060196954756975174, Training accuracy: 0.97265625\n",
      "Test loss: 0.7591525316238403, Test accuracy: 0.789814829826355\n",
      "epoch: 238/300, Training loss: 0.11148717999458313, Training accuracy: 0.96875\n",
      "Test loss: 0.7851874232292175, Test accuracy: 0.800000011920929\n",
      "epoch: 239/300, Training loss: 0.06065928190946579, Training accuracy: 0.97265625\n",
      "Test loss: 0.8598798513412476, Test accuracy: 0.7787036895751953\n",
      "epoch: 240/300, Training loss: 0.029933560639619827, Training accuracy: 0.9755859375\n",
      "Test loss: 0.7680866718292236, Test accuracy: 0.7861111164093018\n",
      "epoch: 241/300, Training loss: 0.07833418250083923, Training accuracy: 0.9765625\n",
      "Test loss: 0.7898672819137573, Test accuracy: 0.7879629731178284\n",
      "epoch: 242/300, Training loss: 0.05137990415096283, Training accuracy: 0.9765625\n",
      "Test loss: 0.8457272052764893, Test accuracy: 0.7740740776062012\n",
      "epoch: 243/300, Training loss: 0.051187533885240555, Training accuracy: 0.974609375\n",
      "Test loss: 0.8081347346305847, Test accuracy: 0.7787036895751953\n",
      "epoch: 244/300, Training loss: 0.07888771593570709, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8223949670791626, Test accuracy: 0.7805555462837219\n",
      "epoch: 245/300, Training loss: 0.04429345205426216, Training accuracy: 0.982421875\n",
      "Test loss: 0.7856828570365906, Test accuracy: 0.7879629731178284\n",
      "epoch: 246/300, Training loss: 0.04241400584578514, Training accuracy: 0.98046875\n",
      "Test loss: 0.8386572599411011, Test accuracy: 0.7787036895751953\n",
      "epoch: 247/300, Training loss: 0.06264656782150269, Training accuracy: 0.9755859375\n",
      "Test loss: 0.80267733335495, Test accuracy: 0.7814814448356628\n",
      "epoch: 248/300, Training loss: 0.1711772382259369, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7947515845298767, Test accuracy: 0.7972221970558167\n",
      "epoch: 249/300, Training loss: 0.08423271030187607, Training accuracy: 0.98046875\n",
      "Test loss: 0.8098371624946594, Test accuracy: 0.7759259343147278\n",
      "epoch: 250/300, Training loss: 0.11169566214084625, Training accuracy: 0.98046875\n",
      "Test loss: 0.7732603549957275, Test accuracy: 0.7879629731178284\n",
      "epoch: 251/300, Training loss: 0.03895026817917824, Training accuracy: 0.9697265625\n",
      "Test loss: 0.8754977583885193, Test accuracy: 0.7712962627410889\n",
      "epoch: 252/300, Training loss: 0.09160179644823074, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8268705606460571, Test accuracy: 0.79537034034729\n",
      "epoch: 253/300, Training loss: 0.05554445460438728, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7922108769416809, Test accuracy: 0.7861111164093018\n",
      "epoch: 254/300, Training loss: 0.16898056864738464, Training accuracy: 0.9677734375\n",
      "Test loss: 0.8338505029678345, Test accuracy: 0.7879629731178284\n",
      "epoch: 255/300, Training loss: 0.1042076125741005, Training accuracy: 0.9716796875\n",
      "Test loss: 0.7568266987800598, Test accuracy: 0.789814829826355\n",
      "epoch: 256/300, Training loss: 0.12885236740112305, Training accuracy: 0.974609375\n",
      "Test loss: 0.8103301525115967, Test accuracy: 0.7861111164093018\n",
      "epoch: 257/300, Training loss: 0.10516537725925446, Training accuracy: 0.978515625\n",
      "Test loss: 0.769018828868866, Test accuracy: 0.7888888716697693\n",
      "epoch: 258/300, Training loss: 0.0309547521173954, Training accuracy: 0.9833984375\n",
      "Test loss: 0.8275629281997681, Test accuracy: 0.7842592597007751\n",
      "epoch: 259/300, Training loss: 0.04815054312348366, Training accuracy: 0.97265625\n",
      "Test loss: 0.8089370131492615, Test accuracy: 0.7879629731178284\n",
      "epoch: 260/300, Training loss: 0.08740568161010742, Training accuracy: 0.9765625\n",
      "Test loss: 0.8315762281417847, Test accuracy: 0.7796295881271362\n",
      "epoch: 261/300, Training loss: 0.04209104925394058, Training accuracy: 0.978515625\n",
      "Test loss: 0.8437944054603577, Test accuracy: 0.7712962627410889\n",
      "epoch: 262/300, Training loss: 0.09008725732564926, Training accuracy: 0.978515625\n",
      "Test loss: 0.8676644563674927, Test accuracy: 0.7685185074806213\n",
      "epoch: 263/300, Training loss: 0.1750192940235138, Training accuracy: 0.9765625\n",
      "Test loss: 0.8199780583381653, Test accuracy: 0.7796295881271362\n",
      "epoch: 264/300, Training loss: 0.05329801142215729, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7909196615219116, Test accuracy: 0.7944444417953491\n",
      "epoch: 265/300, Training loss: 0.056670330464839935, Training accuracy: 0.97265625\n",
      "Test loss: 0.8268365859985352, Test accuracy: 0.79537034034729\n",
      "epoch: 266/300, Training loss: 0.13153192400932312, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8535717725753784, Test accuracy: 0.7777777910232544\n",
      "epoch: 267/300, Training loss: 0.04310613498091698, Training accuracy: 0.9736328125\n",
      "Test loss: 0.800097644329071, Test accuracy: 0.7805555462837219\n",
      "epoch: 268/300, Training loss: 0.05312027409672737, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8278740644454956, Test accuracy: 0.7870370149612427\n",
      "epoch: 269/300, Training loss: 0.041076768189668655, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7953916192054749, Test accuracy: 0.7888888716697693\n",
      "epoch: 270/300, Training loss: 0.053819600492715836, Training accuracy: 0.98046875\n",
      "Test loss: 0.7844589948654175, Test accuracy: 0.7861111164093018\n",
      "epoch: 271/300, Training loss: 0.058926451951265335, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8797649145126343, Test accuracy: 0.7675925493240356\n",
      "epoch: 272/300, Training loss: 0.05862682685256004, Training accuracy: 0.9638671875\n",
      "Test loss: 0.8266868591308594, Test accuracy: 0.7722222208976746\n",
      "epoch: 273/300, Training loss: 0.09155353158712387, Training accuracy: 0.9765625\n",
      "Test loss: 0.8208659291267395, Test accuracy: 0.7777777910232544\n",
      "epoch: 274/300, Training loss: 0.09931936860084534, Training accuracy: 0.970703125\n",
      "Test loss: 0.8325443863868713, Test accuracy: 0.7851851582527161\n",
      "epoch: 275/300, Training loss: 0.04638207331299782, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8013955950737, Test accuracy: 0.7851851582527161\n",
      "epoch: 276/300, Training loss: 0.09165062010288239, Training accuracy: 0.97265625\n",
      "Test loss: 0.8147121071815491, Test accuracy: 0.7768518328666687\n",
      "epoch: 277/300, Training loss: 0.07330171763896942, Training accuracy: 0.96484375\n",
      "Test loss: 0.7607820630073547, Test accuracy: 0.7851851582527161\n",
      "epoch: 278/300, Training loss: 0.06583689898252487, Training accuracy: 0.970703125\n",
      "Test loss: 0.8618406057357788, Test accuracy: 0.7759259343147278\n",
      "epoch: 279/300, Training loss: 0.05024419352412224, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7974215745925903, Test accuracy: 0.789814829826355\n",
      "epoch: 280/300, Training loss: 0.08883751928806305, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8455927968025208, Test accuracy: 0.7842592597007751\n",
      "epoch: 281/300, Training loss: 0.06432203203439713, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8200076222419739, Test accuracy: 0.7879629731178284\n",
      "epoch: 282/300, Training loss: 0.08806710690259933, Training accuracy: 0.978515625\n",
      "Test loss: 0.7779751420021057, Test accuracy: 0.7907407283782959\n",
      "epoch: 283/300, Training loss: 0.07552922517061234, Training accuracy: 0.978515625\n",
      "Test loss: 0.7794381976127625, Test accuracy: 0.7925925850868225\n",
      "epoch: 284/300, Training loss: 0.08666527271270752, Training accuracy: 0.978515625\n",
      "Test loss: 0.8410928249359131, Test accuracy: 0.7777777910232544\n",
      "epoch: 285/300, Training loss: 0.08534044027328491, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8026103377342224, Test accuracy: 0.7907407283782959\n",
      "epoch: 286/300, Training loss: 0.026766879484057426, Training accuracy: 0.98046875\n",
      "Test loss: 0.8497852087020874, Test accuracy: 0.7759259343147278\n",
      "epoch: 287/300, Training loss: 0.08321257680654526, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8087255954742432, Test accuracy: 0.7879629731178284\n",
      "epoch: 288/300, Training loss: 0.12572824954986572, Training accuracy: 0.95703125\n",
      "Test loss: 0.7884280681610107, Test accuracy: 0.7851851582527161\n",
      "epoch: 289/300, Training loss: 0.08730607479810715, Training accuracy: 0.9765625\n",
      "Test loss: 0.8396027088165283, Test accuracy: 0.7870370149612427\n",
      "epoch: 290/300, Training loss: 0.053840965032577515, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8031754493713379, Test accuracy: 0.7879629731178284\n",
      "epoch: 291/300, Training loss: 0.10433097928762436, Training accuracy: 0.974609375\n",
      "Test loss: 0.8689827919006348, Test accuracy: 0.7814814448356628\n",
      "epoch: 292/300, Training loss: 0.07551438361406326, Training accuracy: 0.9765625\n",
      "Test loss: 0.7865323424339294, Test accuracy: 0.7787036895751953\n",
      "epoch: 293/300, Training loss: 0.05838387832045555, Training accuracy: 0.9853515625\n",
      "Test loss: 0.8103365898132324, Test accuracy: 0.7870370149612427\n",
      "epoch: 294/300, Training loss: 0.08713539689779282, Training accuracy: 0.974609375\n",
      "Test loss: 0.8670193552970886, Test accuracy: 0.7925925850868225\n",
      "epoch: 295/300, Training loss: 0.06901732832193375, Training accuracy: 0.984375\n",
      "Test loss: 0.7935248613357544, Test accuracy: 0.7851851582527161\n",
      "epoch: 296/300, Training loss: 0.11374549567699432, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8311386108398438, Test accuracy: 0.7805555462837219\n",
      "epoch: 297/300, Training loss: 0.10283096134662628, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8264710307121277, Test accuracy: 0.7842592597007751\n",
      "epoch: 298/300, Training loss: 0.08408640325069427, Training accuracy: 0.982421875\n",
      "Test loss: 0.7800756692886353, Test accuracy: 0.7907407283782959\n",
      "epoch: 299/300, Training loss: 0.01739514246582985, Training accuracy: 0.982421875\n",
      "Test loss: 0.8024048805236816, Test accuracy: 0.7833333015441895\n",
      "epoch: 300/300, Training loss: 0.10061892867088318, Training accuracy: 0.9765625\n",
      "Test loss: 0.8174723982810974, Test accuracy: 0.7768518328666687\n",
      "best acc:  tensor(0.8037, device='cuda:0')\n",
      "epoch: 1/300, Training loss: 0.5807281732559204, Training accuracy: 0.654296875\n",
      "Test loss: 0.6096546053886414, Test accuracy: 0.6768518686294556\n",
      "epoch: 2/300, Training loss: 0.5113589763641357, Training accuracy: 0.6904296875\n",
      "Test loss: 0.5774822235107422, Test accuracy: 0.6944444179534912\n",
      "epoch: 3/300, Training loss: 0.356012225151062, Training accuracy: 0.7578125\n",
      "Test loss: 0.5427036881446838, Test accuracy: 0.7268518209457397\n",
      "epoch: 4/300, Training loss: 0.46872344613075256, Training accuracy: 0.7734375\n",
      "Test loss: 0.557482898235321, Test accuracy: 0.7212963104248047\n",
      "epoch: 5/300, Training loss: 0.41707032918930054, Training accuracy: 0.78125\n",
      "Test loss: 0.5781184434890747, Test accuracy: 0.7148147821426392\n",
      "epoch: 6/300, Training loss: 0.5278643369674683, Training accuracy: 0.7734375\n",
      "Test loss: 0.5671772956848145, Test accuracy: 0.7212963104248047\n",
      "epoch: 7/300, Training loss: 0.48356372117996216, Training accuracy: 0.7998046875\n",
      "Test loss: 0.6006250977516174, Test accuracy: 0.7277777791023254\n",
      "epoch: 8/300, Training loss: 0.4399656653404236, Training accuracy: 0.7939453125\n",
      "Test loss: 0.5735153555870056, Test accuracy: 0.7194444537162781\n",
      "epoch: 9/300, Training loss: 0.50420743227005, Training accuracy: 0.7900390625\n",
      "Test loss: 0.558245062828064, Test accuracy: 0.7212963104248047\n",
      "epoch: 10/300, Training loss: 0.3893609046936035, Training accuracy: 0.7978515625\n",
      "Test loss: 0.5958619713783264, Test accuracy: 0.7166666388511658\n",
      "epoch: 11/300, Training loss: 0.3319748640060425, Training accuracy: 0.8125\n",
      "Test loss: 0.5677129626274109, Test accuracy: 0.7351851463317871\n",
      "epoch: 12/300, Training loss: 0.2842700183391571, Training accuracy: 0.8076171875\n",
      "Test loss: 0.5660950541496277, Test accuracy: 0.7444444298744202\n",
      "epoch: 13/300, Training loss: 0.45504701137542725, Training accuracy: 0.81640625\n",
      "Test loss: 0.5827978253364563, Test accuracy: 0.729629635810852\n",
      "epoch: 14/300, Training loss: 0.34710365533828735, Training accuracy: 0.833984375\n",
      "Test loss: 0.5827783942222595, Test accuracy: 0.7425925731658936\n",
      "epoch: 15/300, Training loss: 0.3384542465209961, Training accuracy: 0.849609375\n",
      "Test loss: 0.5451773405075073, Test accuracy: 0.7620370388031006\n",
      "epoch: 16/300, Training loss: 0.30103015899658203, Training accuracy: 0.8681640625\n",
      "Test loss: 0.5674673318862915, Test accuracy: 0.7481481432914734\n",
      "epoch: 17/300, Training loss: 0.23122288286685944, Training accuracy: 0.8779296875\n",
      "Test loss: 0.5605347752571106, Test accuracy: 0.7527777552604675\n",
      "epoch: 18/300, Training loss: 0.3811943233013153, Training accuracy: 0.8779296875\n",
      "Test loss: 0.5594493746757507, Test accuracy: 0.7648147940635681\n",
      "epoch: 19/300, Training loss: 0.21040470898151398, Training accuracy: 0.853515625\n",
      "Test loss: 0.5581697225570679, Test accuracy: 0.75\n",
      "epoch: 20/300, Training loss: 0.2287512719631195, Training accuracy: 0.880859375\n",
      "Test loss: 0.5730042457580566, Test accuracy: 0.7611110806465149\n",
      "epoch: 21/300, Training loss: 0.22172582149505615, Training accuracy: 0.880859375\n",
      "Test loss: 0.5835365653038025, Test accuracy: 0.7638888955116272\n",
      "epoch: 22/300, Training loss: 0.3336135745048523, Training accuracy: 0.884765625\n",
      "Test loss: 0.5985925197601318, Test accuracy: 0.7527777552604675\n",
      "epoch: 23/300, Training loss: 0.3030836582183838, Training accuracy: 0.8935546875\n",
      "Test loss: 0.5974687337875366, Test accuracy: 0.760185182094574\n",
      "epoch: 24/300, Training loss: 0.2500499188899994, Training accuracy: 0.890625\n",
      "Test loss: 0.6517598032951355, Test accuracy: 0.7481481432914734\n",
      "epoch: 25/300, Training loss: 0.20132875442504883, Training accuracy: 0.892578125\n",
      "Test loss: 0.5675323009490967, Test accuracy: 0.7768518328666687\n",
      "epoch: 26/300, Training loss: 0.2247997671365738, Training accuracy: 0.8896484375\n",
      "Test loss: 0.6406429409980774, Test accuracy: 0.760185182094574\n",
      "epoch: 27/300, Training loss: 0.21181343495845795, Training accuracy: 0.9072265625\n",
      "Test loss: 0.6635133028030396, Test accuracy: 0.7675925493240356\n",
      "epoch: 28/300, Training loss: 0.20292238891124725, Training accuracy: 0.8916015625\n",
      "Test loss: 0.6204054355621338, Test accuracy: 0.7592592239379883\n",
      "epoch: 29/300, Training loss: 0.17365051805973053, Training accuracy: 0.8935546875\n",
      "Test loss: 0.7037767767906189, Test accuracy: 0.7564814686775208\n",
      "epoch: 30/300, Training loss: 0.2653815448284149, Training accuracy: 0.888671875\n",
      "Test loss: 0.6518422961235046, Test accuracy: 0.7657407522201538\n",
      "epoch: 31/300, Training loss: 0.24502527713775635, Training accuracy: 0.9189453125\n",
      "Test loss: 0.6156088709831238, Test accuracy: 0.7685185074806213\n",
      "epoch: 32/300, Training loss: 0.23268234729766846, Training accuracy: 0.921875\n",
      "Test loss: 0.6280233860015869, Test accuracy: 0.7731481194496155\n",
      "epoch: 33/300, Training loss: 0.21186254918575287, Training accuracy: 0.919921875\n",
      "Test loss: 0.6726399660110474, Test accuracy: 0.7509258985519409\n",
      "epoch: 34/300, Training loss: 0.2857138216495514, Training accuracy: 0.9169921875\n",
      "Test loss: 0.6169248223304749, Test accuracy: 0.7722222208976746\n",
      "epoch: 35/300, Training loss: 0.1714727133512497, Training accuracy: 0.9189453125\n",
      "Test loss: 0.6298945546150208, Test accuracy: 0.7759259343147278\n",
      "epoch: 36/300, Training loss: 0.15914809703826904, Training accuracy: 0.9296875\n",
      "Test loss: 0.6869789361953735, Test accuracy: 0.7777777910232544\n",
      "epoch: 37/300, Training loss: 0.17507462203502655, Training accuracy: 0.947265625\n",
      "Test loss: 0.6856880784034729, Test accuracy: 0.7574073672294617\n",
      "epoch: 38/300, Training loss: 0.1816847324371338, Training accuracy: 0.923828125\n",
      "Test loss: 0.6851024031639099, Test accuracy: 0.7638888955116272\n",
      "epoch: 39/300, Training loss: 0.10780398547649384, Training accuracy: 0.93359375\n",
      "Test loss: 0.6764165163040161, Test accuracy: 0.7657407522201538\n",
      "epoch: 40/300, Training loss: 0.2524038851261139, Training accuracy: 0.923828125\n",
      "Test loss: 0.6844460964202881, Test accuracy: 0.7638888955116272\n",
      "epoch: 41/300, Training loss: 0.23322515189647675, Training accuracy: 0.947265625\n",
      "Test loss: 0.736567497253418, Test accuracy: 0.7472221851348877\n",
      "epoch: 42/300, Training loss: 0.08105991035699844, Training accuracy: 0.9482421875\n",
      "Test loss: 0.728405773639679, Test accuracy: 0.7490740418434143\n",
      "epoch: 43/300, Training loss: 0.16043983399868011, Training accuracy: 0.955078125\n",
      "Test loss: 0.6970576643943787, Test accuracy: 0.7740740776062012\n",
      "epoch: 44/300, Training loss: 0.1583113670349121, Training accuracy: 0.9462890625\n",
      "Test loss: 0.7545483708381653, Test accuracy: 0.7694444060325623\n",
      "epoch: 45/300, Training loss: 0.1503058671951294, Training accuracy: 0.9384765625\n",
      "Test loss: 0.716916561126709, Test accuracy: 0.7564814686775208\n",
      "epoch: 46/300, Training loss: 0.11000051349401474, Training accuracy: 0.9580078125\n",
      "Test loss: 0.7449489831924438, Test accuracy: 0.7768518328666687\n",
      "epoch: 47/300, Training loss: 0.20710091292858124, Training accuracy: 0.9453125\n",
      "Test loss: 0.7589683532714844, Test accuracy: 0.7666666507720947\n",
      "epoch: 48/300, Training loss: 0.13878989219665527, Training accuracy: 0.9609375\n",
      "Test loss: 0.7571949362754822, Test accuracy: 0.7638888955116272\n",
      "epoch: 49/300, Training loss: 0.09056209772825241, Training accuracy: 0.9482421875\n",
      "Test loss: 0.8323767781257629, Test accuracy: 0.7435185313224792\n",
      "epoch: 50/300, Training loss: 0.16275782883167267, Training accuracy: 0.947265625\n",
      "Test loss: 0.7788606882095337, Test accuracy: 0.7777777910232544\n",
      "epoch: 51/300, Training loss: 0.1694725900888443, Training accuracy: 0.939453125\n",
      "Test loss: 0.7762812376022339, Test accuracy: 0.7777777910232544\n",
      "epoch: 52/300, Training loss: 0.18355390429496765, Training accuracy: 0.9482421875\n",
      "Test loss: 0.7742478847503662, Test accuracy: 0.7666666507720947\n",
      "epoch: 53/300, Training loss: 0.16312137246131897, Training accuracy: 0.9541015625\n",
      "Test loss: 0.8088896870613098, Test accuracy: 0.7722222208976746\n",
      "epoch: 54/300, Training loss: 0.09211084246635437, Training accuracy: 0.9541015625\n",
      "Test loss: 0.8251772522926331, Test accuracy: 0.770370364189148\n",
      "epoch: 55/300, Training loss: 0.09820891171693802, Training accuracy: 0.947265625\n",
      "Test loss: 0.7383666038513184, Test accuracy: 0.7749999761581421\n",
      "epoch: 56/300, Training loss: 0.09036427736282349, Training accuracy: 0.9619140625\n",
      "Test loss: 0.8056320548057556, Test accuracy: 0.760185182094574\n",
      "epoch: 57/300, Training loss: 0.1171659454703331, Training accuracy: 0.953125\n",
      "Test loss: 0.7781076431274414, Test accuracy: 0.7657407522201538\n",
      "epoch: 58/300, Training loss: 0.2470318078994751, Training accuracy: 0.9443359375\n",
      "Test loss: 0.7899404764175415, Test accuracy: 0.7629629373550415\n",
      "epoch: 59/300, Training loss: 0.13322849571704865, Training accuracy: 0.96875\n",
      "Test loss: 0.7635589838027954, Test accuracy: 0.7759259343147278\n",
      "epoch: 60/300, Training loss: 0.12657824158668518, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7703031897544861, Test accuracy: 0.7666666507720947\n",
      "epoch: 61/300, Training loss: 0.1269671618938446, Training accuracy: 0.9609375\n",
      "Test loss: 0.7966452240943909, Test accuracy: 0.7527777552604675\n",
      "epoch: 62/300, Training loss: 0.09656258672475815, Training accuracy: 0.9619140625\n",
      "Test loss: 0.7612971067428589, Test accuracy: 0.7685185074806213\n",
      "epoch: 63/300, Training loss: 0.06719478219747543, Training accuracy: 0.958984375\n",
      "Test loss: 0.8286880254745483, Test accuracy: 0.7657407522201538\n",
      "epoch: 64/300, Training loss: 0.08768737316131592, Training accuracy: 0.9609375\n",
      "Test loss: 0.7476765513420105, Test accuracy: 0.7851851582527161\n",
      "epoch: 65/300, Training loss: 0.08670247346162796, Training accuracy: 0.970703125\n",
      "Test loss: 0.794355571269989, Test accuracy: 0.7555555701255798\n",
      "epoch: 66/300, Training loss: 0.12151912599802017, Training accuracy: 0.9541015625\n",
      "Test loss: 0.7930970788002014, Test accuracy: 0.7749999761581421\n",
      "epoch: 67/300, Training loss: 0.05609666556119919, Training accuracy: 0.9619140625\n",
      "Test loss: 0.8372629284858704, Test accuracy: 0.7648147940635681\n",
      "epoch: 68/300, Training loss: 0.11179050803184509, Training accuracy: 0.96875\n",
      "Test loss: 0.8087485432624817, Test accuracy: 0.7777777910232544\n",
      "epoch: 69/300, Training loss: 0.1733202189207077, Training accuracy: 0.9638671875\n",
      "Test loss: 0.7941631078720093, Test accuracy: 0.770370364189148\n",
      "epoch: 70/300, Training loss: 0.10966858267784119, Training accuracy: 0.970703125\n",
      "Test loss: 0.8798673748970032, Test accuracy: 0.7657407522201538\n",
      "epoch: 71/300, Training loss: 0.11431986838579178, Training accuracy: 0.9716796875\n",
      "Test loss: 0.7783094644546509, Test accuracy: 0.7759259343147278\n",
      "epoch: 72/300, Training loss: 0.18031059205532074, Training accuracy: 0.9609375\n",
      "Test loss: 0.8392227292060852, Test accuracy: 0.7666666507720947\n",
      "epoch: 73/300, Training loss: 0.09575346857309341, Training accuracy: 0.96484375\n",
      "Test loss: 0.7707869410514832, Test accuracy: 0.7740740776062012\n",
      "epoch: 74/300, Training loss: 0.07241992652416229, Training accuracy: 0.96875\n",
      "Test loss: 0.8350686430931091, Test accuracy: 0.7740740776062012\n",
      "epoch: 75/300, Training loss: 0.13657566905021667, Training accuracy: 0.9658203125\n",
      "Test loss: 0.8198838829994202, Test accuracy: 0.7731481194496155\n",
      "epoch: 76/300, Training loss: 0.06575510650873184, Training accuracy: 0.9697265625\n",
      "Test loss: 0.8355812430381775, Test accuracy: 0.7620370388031006\n",
      "epoch: 77/300, Training loss: 0.11929357796907425, Training accuracy: 0.9560546875\n",
      "Test loss: 0.819594144821167, Test accuracy: 0.7731481194496155\n",
      "epoch: 78/300, Training loss: 0.048833321779966354, Training accuracy: 0.978515625\n",
      "Test loss: 0.8318559527397156, Test accuracy: 0.770370364189148\n",
      "epoch: 79/300, Training loss: 0.08283228427171707, Training accuracy: 0.96875\n",
      "Test loss: 0.7951743006706238, Test accuracy: 0.7759259343147278\n",
      "epoch: 80/300, Training loss: 0.2370537668466568, Training accuracy: 0.962890625\n",
      "Test loss: 0.8316501975059509, Test accuracy: 0.7777777910232544\n",
      "epoch: 81/300, Training loss: 0.10545799136161804, Training accuracy: 0.962890625\n",
      "Test loss: 0.8072771430015564, Test accuracy: 0.7814814448356628\n",
      "epoch: 82/300, Training loss: 0.11452136188745499, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8291857242584229, Test accuracy: 0.770370364189148\n",
      "epoch: 83/300, Training loss: 0.08356587588787079, Training accuracy: 0.966796875\n",
      "Test loss: 0.805567741394043, Test accuracy: 0.7805555462837219\n",
      "epoch: 84/300, Training loss: 0.09527314454317093, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7999078035354614, Test accuracy: 0.7777777910232544\n",
      "epoch: 85/300, Training loss: 0.13895536959171295, Training accuracy: 0.96484375\n",
      "Test loss: 0.818440318107605, Test accuracy: 0.7842592597007751\n",
      "epoch: 86/300, Training loss: 0.07599159330129623, Training accuracy: 0.9765625\n",
      "Test loss: 0.869012176990509, Test accuracy: 0.7805555462837219\n",
      "epoch: 87/300, Training loss: 0.11612601578235626, Training accuracy: 0.9658203125\n",
      "Test loss: 0.9041048288345337, Test accuracy: 0.7685185074806213\n",
      "epoch: 88/300, Training loss: 0.04044754430651665, Training accuracy: 0.970703125\n",
      "Test loss: 0.8437294960021973, Test accuracy: 0.7648147940635681\n",
      "epoch: 89/300, Training loss: 0.046863630414009094, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8066657781600952, Test accuracy: 0.7731481194496155\n",
      "epoch: 90/300, Training loss: 0.09307781606912613, Training accuracy: 0.9765625\n",
      "Test loss: 0.824806272983551, Test accuracy: 0.7731481194496155\n",
      "epoch: 91/300, Training loss: 0.13872763514518738, Training accuracy: 0.970703125\n",
      "Test loss: 0.828187882900238, Test accuracy: 0.7731481194496155\n",
      "epoch: 92/300, Training loss: 0.06155553460121155, Training accuracy: 0.966796875\n",
      "Test loss: 0.8267928957939148, Test accuracy: 0.7787036895751953\n",
      "epoch: 93/300, Training loss: 0.04748217388987541, Training accuracy: 0.9697265625\n",
      "Test loss: 0.80894535779953, Test accuracy: 0.7777777910232544\n",
      "epoch: 94/300, Training loss: 0.09762580692768097, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8363003730773926, Test accuracy: 0.7777777910232544\n",
      "epoch: 95/300, Training loss: 0.13491806387901306, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7994987964630127, Test accuracy: 0.770370364189148\n",
      "epoch: 96/300, Training loss: 0.05357140302658081, Training accuracy: 0.974609375\n",
      "Test loss: 0.8418571949005127, Test accuracy: 0.7833333015441895\n",
      "epoch: 97/300, Training loss: 0.057073287665843964, Training accuracy: 0.97265625\n",
      "Test loss: 0.8637526631355286, Test accuracy: 0.7722222208976746\n",
      "epoch: 98/300, Training loss: 0.08480630069971085, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8326323628425598, Test accuracy: 0.7842592597007751\n",
      "epoch: 99/300, Training loss: 0.03692992404103279, Training accuracy: 0.970703125\n",
      "Test loss: 0.8434540629386902, Test accuracy: 0.7759259343147278\n",
      "epoch: 100/300, Training loss: 0.09132988750934601, Training accuracy: 0.978515625\n",
      "Test loss: 0.8340995907783508, Test accuracy: 0.7675925493240356\n",
      "epoch: 101/300, Training loss: 0.10675892233848572, Training accuracy: 0.9716796875\n",
      "Test loss: 0.7981809377670288, Test accuracy: 0.7777777910232544\n",
      "epoch: 102/300, Training loss: 0.12741316854953766, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8009625673294067, Test accuracy: 0.7851851582527161\n",
      "epoch: 103/300, Training loss: 0.03924310952425003, Training accuracy: 0.970703125\n",
      "Test loss: 0.8129554390907288, Test accuracy: 0.7722222208976746\n",
      "epoch: 104/300, Training loss: 0.08172506093978882, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7958084344863892, Test accuracy: 0.7814814448356628\n",
      "epoch: 105/300, Training loss: 0.06328894942998886, Training accuracy: 0.9765625\n",
      "Test loss: 0.815945565700531, Test accuracy: 0.7694444060325623\n",
      "epoch: 106/300, Training loss: 0.05485805869102478, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7978300452232361, Test accuracy: 0.789814829826355\n",
      "epoch: 107/300, Training loss: 0.0464693121612072, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8334599137306213, Test accuracy: 0.7787036895751953\n",
      "epoch: 108/300, Training loss: 0.055437877774238586, Training accuracy: 0.9765625\n",
      "Test loss: 0.8044205904006958, Test accuracy: 0.7759259343147278\n",
      "epoch: 109/300, Training loss: 0.05669776350259781, Training accuracy: 0.97265625\n",
      "Test loss: 0.871536374092102, Test accuracy: 0.7759259343147278\n",
      "epoch: 110/300, Training loss: 0.06990199536085129, Training accuracy: 0.9658203125\n",
      "Test loss: 0.8170245885848999, Test accuracy: 0.7861111164093018\n",
      "epoch: 111/300, Training loss: 0.07584965974092484, Training accuracy: 0.9638671875\n",
      "Test loss: 0.7790791988372803, Test accuracy: 0.7888888716697693\n",
      "epoch: 112/300, Training loss: 0.02823389135301113, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8199416399002075, Test accuracy: 0.7861111164093018\n",
      "epoch: 113/300, Training loss: 0.11154613643884659, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8420920968055725, Test accuracy: 0.7722222208976746\n",
      "epoch: 114/300, Training loss: 0.09760908037424088, Training accuracy: 0.9716796875\n",
      "Test loss: 0.842789888381958, Test accuracy: 0.7694444060325623\n",
      "epoch: 115/300, Training loss: 0.09735991060733795, Training accuracy: 0.97265625\n",
      "Test loss: 0.7870169281959534, Test accuracy: 0.7731481194496155\n",
      "epoch: 116/300, Training loss: 0.0732797235250473, Training accuracy: 0.9658203125\n",
      "Test loss: 0.8193674087524414, Test accuracy: 0.7694444060325623\n",
      "epoch: 117/300, Training loss: 0.07900404930114746, Training accuracy: 0.978515625\n",
      "Test loss: 0.8097644448280334, Test accuracy: 0.7777777910232544\n",
      "epoch: 118/300, Training loss: 0.17830099165439606, Training accuracy: 0.9638671875\n",
      "Test loss: 0.8286705017089844, Test accuracy: 0.7694444060325623\n",
      "epoch: 119/300, Training loss: 0.06019454449415207, Training accuracy: 0.974609375\n",
      "Test loss: 0.8166698813438416, Test accuracy: 0.7749999761581421\n",
      "epoch: 120/300, Training loss: 0.0629536584019661, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8299760222434998, Test accuracy: 0.7879629731178284\n",
      "epoch: 121/300, Training loss: 0.0792846754193306, Training accuracy: 0.98046875\n",
      "Test loss: 0.7828363180160522, Test accuracy: 0.7805555462837219\n",
      "epoch: 122/300, Training loss: 0.05591800436377525, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7996885776519775, Test accuracy: 0.7833333015441895\n",
      "epoch: 123/300, Training loss: 0.11311396956443787, Training accuracy: 0.96484375\n",
      "Test loss: 0.8070771098136902, Test accuracy: 0.7666666507720947\n",
      "epoch: 124/300, Training loss: 0.05609683319926262, Training accuracy: 0.966796875\n",
      "Test loss: 0.8576694130897522, Test accuracy: 0.7759259343147278\n",
      "epoch: 125/300, Training loss: 0.0240202397108078, Training accuracy: 0.9765625\n",
      "Test loss: 0.9072510600090027, Test accuracy: 0.7796295881271362\n",
      "epoch: 126/300, Training loss: 0.06511695683002472, Training accuracy: 0.970703125\n",
      "Test loss: 0.8118494153022766, Test accuracy: 0.7740740776062012\n",
      "epoch: 127/300, Training loss: 0.08249100297689438, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7808108329772949, Test accuracy: 0.7796295881271362\n",
      "epoch: 128/300, Training loss: 0.08717837184667587, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8702567219734192, Test accuracy: 0.7555555701255798\n",
      "epoch: 129/300, Training loss: 0.08720503002405167, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8367003202438354, Test accuracy: 0.7666666507720947\n",
      "epoch: 130/300, Training loss: 0.0715223029255867, Training accuracy: 0.9697265625\n",
      "Test loss: 0.8087537288665771, Test accuracy: 0.7824074029922485\n",
      "epoch: 131/300, Training loss: 0.07551079988479614, Training accuracy: 0.97265625\n",
      "Test loss: 0.7822642922401428, Test accuracy: 0.7935184836387634\n",
      "epoch: 132/300, Training loss: 0.06269392371177673, Training accuracy: 0.97265625\n",
      "Test loss: 0.818244457244873, Test accuracy: 0.7805555462837219\n",
      "epoch: 133/300, Training loss: 0.05991456285119057, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8512382507324219, Test accuracy: 0.7648147940635681\n",
      "epoch: 134/300, Training loss: 0.06942538172006607, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8586214780807495, Test accuracy: 0.7796295881271362\n",
      "epoch: 135/300, Training loss: 0.05321584641933441, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8083876371383667, Test accuracy: 0.7759259343147278\n",
      "epoch: 136/300, Training loss: 0.07335969805717468, Training accuracy: 0.970703125\n",
      "Test loss: 0.8611830472946167, Test accuracy: 0.770370364189148\n",
      "epoch: 137/300, Training loss: 0.04042681306600571, Training accuracy: 0.974609375\n",
      "Test loss: 0.7868291139602661, Test accuracy: 0.7888888716697693\n",
      "epoch: 138/300, Training loss: 0.06794828176498413, Training accuracy: 0.97265625\n",
      "Test loss: 0.8094725012779236, Test accuracy: 0.7740740776062012\n",
      "epoch: 139/300, Training loss: 0.054182861000299454, Training accuracy: 0.9775390625\n",
      "Test loss: 0.823930561542511, Test accuracy: 0.7749999761581421\n",
      "epoch: 140/300, Training loss: 0.07091640681028366, Training accuracy: 0.9765625\n",
      "Test loss: 0.8560124039649963, Test accuracy: 0.7694444060325623\n",
      "epoch: 141/300, Training loss: 0.03961484506726265, Training accuracy: 0.974609375\n",
      "Test loss: 0.8180540800094604, Test accuracy: 0.7916666269302368\n",
      "epoch: 142/300, Training loss: 0.041388850659132004, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8573766350746155, Test accuracy: 0.7796295881271362\n",
      "epoch: 143/300, Training loss: 0.059349045157432556, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7918828129768372, Test accuracy: 0.7814814448356628\n",
      "epoch: 144/300, Training loss: 0.03270627185702324, Training accuracy: 0.98046875\n",
      "Test loss: 0.838628888130188, Test accuracy: 0.7768518328666687\n",
      "epoch: 145/300, Training loss: 0.16794225573539734, Training accuracy: 0.970703125\n",
      "Test loss: 0.7702959179878235, Test accuracy: 0.7861111164093018\n",
      "epoch: 146/300, Training loss: 0.05702986568212509, Training accuracy: 0.9853515625\n",
      "Test loss: 0.8022141456604004, Test accuracy: 0.7805555462837219\n",
      "epoch: 147/300, Training loss: 0.09949849545955658, Training accuracy: 0.9765625\n",
      "Test loss: 0.871095597743988, Test accuracy: 0.770370364189148\n",
      "epoch: 148/300, Training loss: 0.05224248021841049, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8373525738716125, Test accuracy: 0.7722222208976746\n",
      "epoch: 149/300, Training loss: 0.03425895422697067, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8320347666740417, Test accuracy: 0.7777777910232544\n",
      "epoch: 150/300, Training loss: 0.05815821513533592, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8799396753311157, Test accuracy: 0.7768518328666687\n",
      "epoch: 151/300, Training loss: 0.05460083484649658, Training accuracy: 0.97265625\n",
      "Test loss: 0.798841118812561, Test accuracy: 0.7962962985038757\n",
      "epoch: 152/300, Training loss: 0.07213722169399261, Training accuracy: 0.974609375\n",
      "Test loss: 0.8420392870903015, Test accuracy: 0.7638888955116272\n",
      "epoch: 153/300, Training loss: 0.1155129000544548, Training accuracy: 0.970703125\n",
      "Test loss: 0.8506592512130737, Test accuracy: 0.7787036895751953\n",
      "epoch: 154/300, Training loss: 0.05146074667572975, Training accuracy: 0.982421875\n",
      "Test loss: 0.8585953712463379, Test accuracy: 0.7833333015441895\n",
      "epoch: 155/300, Training loss: 0.06287971138954163, Training accuracy: 0.9765625\n",
      "Test loss: 0.8605238199234009, Test accuracy: 0.7712962627410889\n",
      "epoch: 156/300, Training loss: 0.05665050446987152, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8194281458854675, Test accuracy: 0.7907407283782959\n",
      "epoch: 157/300, Training loss: 0.08069691061973572, Training accuracy: 0.9765625\n",
      "Test loss: 0.848895251750946, Test accuracy: 0.7731481194496155\n",
      "epoch: 158/300, Training loss: 0.187009796500206, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8428037762641907, Test accuracy: 0.7805555462837219\n",
      "epoch: 159/300, Training loss: 0.12806163728237152, Training accuracy: 0.98046875\n",
      "Test loss: 0.8182874917984009, Test accuracy: 0.7694444060325623\n",
      "epoch: 160/300, Training loss: 0.052475593984127045, Training accuracy: 0.9736328125\n",
      "Test loss: 0.9076713919639587, Test accuracy: 0.7787036895751953\n",
      "epoch: 161/300, Training loss: 0.0397661030292511, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8096445798873901, Test accuracy: 0.7787036895751953\n",
      "epoch: 162/300, Training loss: 0.06839275360107422, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8631788492202759, Test accuracy: 0.7796295881271362\n",
      "epoch: 163/300, Training loss: 0.09262362867593765, Training accuracy: 0.9619140625\n",
      "Test loss: 0.8421074151992798, Test accuracy: 0.7731481194496155\n",
      "epoch: 164/300, Training loss: 0.07643964141607285, Training accuracy: 0.9775390625\n",
      "Test loss: 0.838129997253418, Test accuracy: 0.7712962627410889\n",
      "epoch: 165/300, Training loss: 0.12015136331319809, Training accuracy: 0.98046875\n",
      "Test loss: 0.7957370281219482, Test accuracy: 0.7824074029922485\n",
      "epoch: 166/300, Training loss: 0.08389425277709961, Training accuracy: 0.96875\n",
      "Test loss: 0.8069661259651184, Test accuracy: 0.7638888955116272\n",
      "epoch: 167/300, Training loss: 0.04654852673411369, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8709741234779358, Test accuracy: 0.7759259343147278\n",
      "epoch: 168/300, Training loss: 0.10426678508520126, Training accuracy: 0.9765625\n",
      "Test loss: 0.8302739858627319, Test accuracy: 0.7731481194496155\n",
      "epoch: 169/300, Training loss: 0.0912117138504982, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8661379814147949, Test accuracy: 0.770370364189148\n",
      "epoch: 170/300, Training loss: 0.04180900380015373, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8944240212440491, Test accuracy: 0.7777777910232544\n",
      "epoch: 171/300, Training loss: 0.05766289681196213, Training accuracy: 0.982421875\n",
      "Test loss: 0.8533422946929932, Test accuracy: 0.7749999761581421\n",
      "epoch: 172/300, Training loss: 0.0719023197889328, Training accuracy: 0.9765625\n",
      "Test loss: 0.8745166659355164, Test accuracy: 0.7555555701255798\n",
      "epoch: 173/300, Training loss: 0.04188336804509163, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8217197060585022, Test accuracy: 0.7777777910232544\n",
      "epoch: 174/300, Training loss: 0.08489260822534561, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8555846214294434, Test accuracy: 0.7666666507720947\n",
      "epoch: 175/300, Training loss: 0.07237353920936584, Training accuracy: 0.9755859375\n",
      "Test loss: 0.7933627367019653, Test accuracy: 0.7759259343147278\n",
      "epoch: 176/300, Training loss: 0.05864691734313965, Training accuracy: 0.9658203125\n",
      "Test loss: 0.8256661891937256, Test accuracy: 0.7824074029922485\n",
      "epoch: 177/300, Training loss: 0.05906303599476814, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8579694628715515, Test accuracy: 0.7712962627410889\n",
      "epoch: 178/300, Training loss: 0.10439087450504303, Training accuracy: 0.97265625\n",
      "Test loss: 0.8381978869438171, Test accuracy: 0.770370364189148\n",
      "epoch: 179/300, Training loss: 0.1322653442621231, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8125684857368469, Test accuracy: 0.7685185074806213\n",
      "epoch: 180/300, Training loss: 0.058291684836149216, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8121255040168762, Test accuracy: 0.770370364189148\n",
      "epoch: 181/300, Training loss: 0.03534511476755142, Training accuracy: 0.9765625\n",
      "Test loss: 0.7875918745994568, Test accuracy: 0.7842592597007751\n",
      "epoch: 182/300, Training loss: 0.04415114223957062, Training accuracy: 0.97265625\n",
      "Test loss: 0.8678988218307495, Test accuracy: 0.789814829826355\n",
      "epoch: 183/300, Training loss: 0.07889360934495926, Training accuracy: 0.974609375\n",
      "Test loss: 0.8604943156242371, Test accuracy: 0.7805555462837219\n",
      "epoch: 184/300, Training loss: 0.04815319553017616, Training accuracy: 0.97265625\n",
      "Test loss: 0.8933659791946411, Test accuracy: 0.7768518328666687\n",
      "epoch: 185/300, Training loss: 0.0509168766438961, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8043839335441589, Test accuracy: 0.7814814448356628\n",
      "epoch: 186/300, Training loss: 0.07135637104511261, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8285978436470032, Test accuracy: 0.7805555462837219\n",
      "epoch: 187/300, Training loss: 0.029274962842464447, Training accuracy: 0.98046875\n",
      "Test loss: 0.8484463691711426, Test accuracy: 0.7731481194496155\n",
      "epoch: 188/300, Training loss: 0.05141983553767204, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8579512238502502, Test accuracy: 0.7833333015441895\n",
      "epoch: 189/300, Training loss: 0.09899715334177017, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8113365173339844, Test accuracy: 0.7824074029922485\n",
      "epoch: 190/300, Training loss: 0.04544859379529953, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8339262008666992, Test accuracy: 0.7861111164093018\n",
      "epoch: 191/300, Training loss: 0.0601620227098465, Training accuracy: 0.9697265625\n",
      "Test loss: 0.7849879264831543, Test accuracy: 0.7777777910232544\n",
      "epoch: 192/300, Training loss: 0.08768941462039948, Training accuracy: 0.9677734375\n",
      "Test loss: 0.8466017842292786, Test accuracy: 0.7805555462837219\n",
      "epoch: 193/300, Training loss: 0.05049940198659897, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8604313731193542, Test accuracy: 0.7787036895751953\n",
      "epoch: 194/300, Training loss: 0.05689188092947006, Training accuracy: 0.974609375\n",
      "Test loss: 0.8424539566040039, Test accuracy: 0.7777777910232544\n",
      "epoch: 195/300, Training loss: 0.03366199508309364, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8670849800109863, Test accuracy: 0.7712962627410889\n",
      "epoch: 196/300, Training loss: 0.07026886194944382, Training accuracy: 0.974609375\n",
      "Test loss: 0.8167557716369629, Test accuracy: 0.7694444060325623\n",
      "epoch: 197/300, Training loss: 0.06650613993406296, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8473984599113464, Test accuracy: 0.7777777910232544\n",
      "epoch: 198/300, Training loss: 0.1195167824625969, Training accuracy: 0.970703125\n",
      "Test loss: 0.8311849236488342, Test accuracy: 0.7787036895751953\n",
      "epoch: 199/300, Training loss: 0.11139190942049026, Training accuracy: 0.96875\n",
      "Test loss: 0.8203806281089783, Test accuracy: 0.7805555462837219\n",
      "epoch: 200/300, Training loss: 0.04951457679271698, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8964877724647522, Test accuracy: 0.7749999761581421\n",
      "epoch: 201/300, Training loss: 0.09441282600164413, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8174009323120117, Test accuracy: 0.7740740776062012\n",
      "epoch: 202/300, Training loss: 0.04898089915513992, Training accuracy: 0.9873046875\n",
      "Test loss: 0.8382631540298462, Test accuracy: 0.7768518328666687\n",
      "epoch: 203/300, Training loss: 0.07864794880151749, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8212122321128845, Test accuracy: 0.7759259343147278\n",
      "epoch: 204/300, Training loss: 0.03296351805329323, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8555434346199036, Test accuracy: 0.7768518328666687\n",
      "epoch: 205/300, Training loss: 0.02202959917485714, Training accuracy: 0.978515625\n",
      "Test loss: 0.7503902316093445, Test accuracy: 0.7870370149612427\n",
      "epoch: 206/300, Training loss: 0.07296562939882278, Training accuracy: 0.9853515625\n",
      "Test loss: 0.8296579718589783, Test accuracy: 0.7861111164093018\n",
      "epoch: 207/300, Training loss: 0.05097188800573349, Training accuracy: 0.978515625\n",
      "Test loss: 0.8603302240371704, Test accuracy: 0.7666666507720947\n",
      "epoch: 208/300, Training loss: 0.05531902611255646, Training accuracy: 0.978515625\n",
      "Test loss: 0.838252067565918, Test accuracy: 0.7759259343147278\n",
      "epoch: 209/300, Training loss: 0.066853366792202, Training accuracy: 0.974609375\n",
      "Test loss: 0.8191811442375183, Test accuracy: 0.7768518328666687\n",
      "epoch: 210/300, Training loss: 0.04976450651884079, Training accuracy: 0.9736328125\n",
      "Test loss: 0.7881218791007996, Test accuracy: 0.7833333015441895\n",
      "epoch: 211/300, Training loss: 0.07404696941375732, Training accuracy: 0.98046875\n",
      "Test loss: 0.8530895113945007, Test accuracy: 0.7712962627410889\n",
      "epoch: 212/300, Training loss: 0.05042054131627083, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8683693408966064, Test accuracy: 0.7731481194496155\n",
      "epoch: 213/300, Training loss: 0.04277060553431511, Training accuracy: 0.96875\n",
      "Test loss: 0.7836285829544067, Test accuracy: 0.7796295881271362\n",
      "epoch: 214/300, Training loss: 0.11096058040857315, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8397570848464966, Test accuracy: 0.7787036895751953\n",
      "epoch: 215/300, Training loss: 0.09329792112112045, Training accuracy: 0.984375\n",
      "Test loss: 0.9574196338653564, Test accuracy: 0.7657407522201538\n",
      "epoch: 216/300, Training loss: 0.02977236546576023, Training accuracy: 0.978515625\n",
      "Test loss: 0.8326025605201721, Test accuracy: 0.7712962627410889\n",
      "epoch: 217/300, Training loss: 0.046794820576906204, Training accuracy: 0.984375\n",
      "Test loss: 0.8341328501701355, Test accuracy: 0.7740740776062012\n",
      "epoch: 218/300, Training loss: 0.09848343580961227, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8535236716270447, Test accuracy: 0.7833333015441895\n",
      "epoch: 219/300, Training loss: 0.10292577743530273, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8523505926132202, Test accuracy: 0.7675925493240356\n",
      "epoch: 220/300, Training loss: 0.05375850200653076, Training accuracy: 0.974609375\n",
      "Test loss: 0.8474591374397278, Test accuracy: 0.7777777910232544\n",
      "epoch: 221/300, Training loss: 0.07084617763757706, Training accuracy: 0.9775390625\n",
      "Test loss: 0.7820334434509277, Test accuracy: 0.79537034034729\n",
      "epoch: 222/300, Training loss: 0.03989873826503754, Training accuracy: 0.9775390625\n",
      "Test loss: 0.804628312587738, Test accuracy: 0.7907407283782959\n",
      "epoch: 223/300, Training loss: 0.04520712420344353, Training accuracy: 0.9794921875\n",
      "Test loss: 0.7738761901855469, Test accuracy: 0.7842592597007751\n",
      "epoch: 224/300, Training loss: 0.05540116876363754, Training accuracy: 0.978515625\n",
      "Test loss: 0.8500625491142273, Test accuracy: 0.7777777910232544\n",
      "epoch: 225/300, Training loss: 0.10486996173858643, Training accuracy: 0.96875\n",
      "Test loss: 0.8348760008811951, Test accuracy: 0.7759259343147278\n",
      "epoch: 226/300, Training loss: 0.029783084988594055, Training accuracy: 0.986328125\n",
      "Test loss: 0.7955338358879089, Test accuracy: 0.7879629731178284\n",
      "epoch: 227/300, Training loss: 0.04122266545891762, Training accuracy: 0.9833984375\n",
      "Test loss: 0.8682093024253845, Test accuracy: 0.7620370388031006\n",
      "epoch: 228/300, Training loss: 0.07103537768125534, Training accuracy: 0.9677734375\n",
      "Test loss: 0.8321784138679504, Test accuracy: 0.7749999761581421\n",
      "epoch: 229/300, Training loss: 0.034404341131448746, Training accuracy: 0.982421875\n",
      "Test loss: 0.8546852469444275, Test accuracy: 0.7638888955116272\n",
      "epoch: 230/300, Training loss: 0.050620388239622116, Training accuracy: 0.970703125\n",
      "Test loss: 0.8717939257621765, Test accuracy: 0.7833333015441895\n",
      "epoch: 231/300, Training loss: 0.03995848447084427, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8247880935668945, Test accuracy: 0.7861111164093018\n",
      "epoch: 232/300, Training loss: 0.03740154951810837, Training accuracy: 0.9833984375\n",
      "Test loss: 0.8068056702613831, Test accuracy: 0.7787036895751953\n",
      "epoch: 233/300, Training loss: 0.07438426464796066, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8725264072418213, Test accuracy: 0.7675925493240356\n",
      "epoch: 234/300, Training loss: 0.05153626948595047, Training accuracy: 0.982421875\n",
      "Test loss: 0.8576619029045105, Test accuracy: 0.7787036895751953\n",
      "epoch: 235/300, Training loss: 0.148307204246521, Training accuracy: 0.978515625\n",
      "Test loss: 0.856075644493103, Test accuracy: 0.7685185074806213\n",
      "epoch: 236/300, Training loss: 0.08493469655513763, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8439760208129883, Test accuracy: 0.7787036895751953\n",
      "epoch: 237/300, Training loss: 0.07706417143344879, Training accuracy: 0.978515625\n",
      "Test loss: 0.82294762134552, Test accuracy: 0.7768518328666687\n",
      "epoch: 238/300, Training loss: 0.10541769862174988, Training accuracy: 0.978515625\n",
      "Test loss: 0.8650001287460327, Test accuracy: 0.7768518328666687\n",
      "epoch: 239/300, Training loss: 0.07513399422168732, Training accuracy: 0.984375\n",
      "Test loss: 0.8503024578094482, Test accuracy: 0.7768518328666687\n",
      "epoch: 240/300, Training loss: 0.07892443984746933, Training accuracy: 0.9814453125\n",
      "Test loss: 0.8071179389953613, Test accuracy: 0.7907407283782959\n",
      "epoch: 241/300, Training loss: 0.06383566558361053, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8162795901298523, Test accuracy: 0.7796295881271362\n",
      "epoch: 242/300, Training loss: 0.07098843902349472, Training accuracy: 0.970703125\n",
      "Test loss: 0.8515121936798096, Test accuracy: 0.7749999761581421\n",
      "epoch: 243/300, Training loss: 0.05746671184897423, Training accuracy: 0.9765625\n",
      "Test loss: 0.7945414185523987, Test accuracy: 0.7731481194496155\n",
      "epoch: 244/300, Training loss: 0.048089005053043365, Training accuracy: 0.974609375\n",
      "Test loss: 0.8394221663475037, Test accuracy: 0.7842592597007751\n",
      "epoch: 245/300, Training loss: 0.05368795245885849, Training accuracy: 0.9697265625\n",
      "Test loss: 0.8103910088539124, Test accuracy: 0.7759259343147278\n",
      "epoch: 246/300, Training loss: 0.11282126605510712, Training accuracy: 0.978515625\n",
      "Test loss: 0.8073511719703674, Test accuracy: 0.7814814448356628\n",
      "epoch: 247/300, Training loss: 0.04826071113348007, Training accuracy: 0.978515625\n",
      "Test loss: 0.8307303786277771, Test accuracy: 0.7842592597007751\n",
      "epoch: 248/300, Training loss: 0.04745499789714813, Training accuracy: 0.982421875\n",
      "Test loss: 0.8229007720947266, Test accuracy: 0.7824074029922485\n",
      "epoch: 249/300, Training loss: 0.05171072855591774, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8094466328620911, Test accuracy: 0.7805555462837219\n",
      "epoch: 250/300, Training loss: 0.043650563806295395, Training accuracy: 0.9765625\n",
      "Test loss: 0.8242713212966919, Test accuracy: 0.7787036895751953\n",
      "epoch: 251/300, Training loss: 0.09150680899620056, Training accuracy: 0.9755859375\n",
      "Test loss: 0.816269040107727, Test accuracy: 0.7851851582527161\n",
      "epoch: 252/300, Training loss: 0.01927534118294716, Training accuracy: 0.9755859375\n",
      "Test loss: 0.871632993221283, Test accuracy: 0.7749999761581421\n",
      "epoch: 253/300, Training loss: 0.03674415871500969, Training accuracy: 0.978515625\n",
      "Test loss: 0.8346713781356812, Test accuracy: 0.7712962627410889\n",
      "epoch: 254/300, Training loss: 0.13097241520881653, Training accuracy: 0.978515625\n",
      "Test loss: 0.8539323806762695, Test accuracy: 0.7675925493240356\n",
      "epoch: 255/300, Training loss: 0.08483380824327469, Training accuracy: 0.982421875\n",
      "Test loss: 0.8410600423812866, Test accuracy: 0.7787036895751953\n",
      "epoch: 256/300, Training loss: 0.013843800872564316, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8305699825286865, Test accuracy: 0.7907407283782959\n",
      "epoch: 257/300, Training loss: 0.06341630220413208, Training accuracy: 0.978515625\n",
      "Test loss: 0.8769181966781616, Test accuracy: 0.770370364189148\n",
      "epoch: 258/300, Training loss: 0.04043557122349739, Training accuracy: 0.98046875\n",
      "Test loss: 0.8498796224594116, Test accuracy: 0.7740740776062012\n",
      "epoch: 259/300, Training loss: 0.053078629076480865, Training accuracy: 0.96875\n",
      "Test loss: 0.8211233019828796, Test accuracy: 0.7814814448356628\n",
      "epoch: 260/300, Training loss: 0.03099757805466652, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8538424372673035, Test accuracy: 0.7749999761581421\n",
      "epoch: 261/300, Training loss: 0.060397159308195114, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8908243775367737, Test accuracy: 0.7722222208976746\n",
      "epoch: 262/300, Training loss: 0.1036762073636055, Training accuracy: 0.9765625\n",
      "Test loss: 0.8422965407371521, Test accuracy: 0.7666666507720947\n",
      "epoch: 263/300, Training loss: 0.0749293714761734, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8704100251197815, Test accuracy: 0.7740740776062012\n",
      "epoch: 264/300, Training loss: 0.024576226249337196, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8569403886795044, Test accuracy: 0.7777777910232544\n",
      "epoch: 265/300, Training loss: 0.043631527572870255, Training accuracy: 0.9765625\n",
      "Test loss: 0.8786569833755493, Test accuracy: 0.7722222208976746\n",
      "epoch: 266/300, Training loss: 0.09516452997922897, Training accuracy: 0.97265625\n",
      "Test loss: 0.8574244379997253, Test accuracy: 0.7740740776062012\n",
      "epoch: 267/300, Training loss: 0.047493185847997665, Training accuracy: 0.9853515625\n",
      "Test loss: 0.8558388948440552, Test accuracy: 0.7888888716697693\n",
      "epoch: 268/300, Training loss: 0.05334841459989548, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8257849812507629, Test accuracy: 0.7824074029922485\n",
      "epoch: 269/300, Training loss: 0.0798531100153923, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8482069969177246, Test accuracy: 0.7722222208976746\n",
      "epoch: 270/300, Training loss: 0.055553533136844635, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8022536635398865, Test accuracy: 0.7888888716697693\n",
      "epoch: 271/300, Training loss: 0.062282003462314606, Training accuracy: 0.97265625\n",
      "Test loss: 0.819455087184906, Test accuracy: 0.7712962627410889\n",
      "epoch: 272/300, Training loss: 0.16197849810123444, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8255617022514343, Test accuracy: 0.7805555462837219\n",
      "epoch: 273/300, Training loss: 0.12970104813575745, Training accuracy: 0.96484375\n",
      "Test loss: 0.8837648630142212, Test accuracy: 0.770370364189148\n",
      "epoch: 274/300, Training loss: 0.037857115268707275, Training accuracy: 0.98046875\n",
      "Test loss: 0.8550182580947876, Test accuracy: 0.7861111164093018\n",
      "epoch: 275/300, Training loss: 0.05103679746389389, Training accuracy: 0.96484375\n",
      "Test loss: 0.8591218590736389, Test accuracy: 0.7694444060325623\n",
      "epoch: 276/300, Training loss: 0.11841236054897308, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8652676343917847, Test accuracy: 0.7712962627410889\n",
      "epoch: 277/300, Training loss: 0.10076890885829926, Training accuracy: 0.9716796875\n",
      "Test loss: 0.849689781665802, Test accuracy: 0.7777777910232544\n",
      "epoch: 278/300, Training loss: 0.07029362767934799, Training accuracy: 0.970703125\n",
      "Test loss: 0.8939811587333679, Test accuracy: 0.7592592239379883\n",
      "epoch: 279/300, Training loss: 0.06482730060815811, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8609302639961243, Test accuracy: 0.7824074029922485\n",
      "epoch: 280/300, Training loss: 0.052944961935281754, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8654833436012268, Test accuracy: 0.7768518328666687\n",
      "epoch: 281/300, Training loss: 0.023262761533260345, Training accuracy: 0.984375\n",
      "Test loss: 0.8969056010246277, Test accuracy: 0.7657407522201538\n",
      "epoch: 282/300, Training loss: 0.0748186782002449, Training accuracy: 0.98046875\n",
      "Test loss: 0.7986858487129211, Test accuracy: 0.7962962985038757\n",
      "epoch: 283/300, Training loss: 0.12202450633049011, Training accuracy: 0.98046875\n",
      "Test loss: 0.8568278551101685, Test accuracy: 0.7814814448356628\n",
      "epoch: 284/300, Training loss: 0.03517429158091545, Training accuracy: 0.9765625\n",
      "Test loss: 0.8618242740631104, Test accuracy: 0.7712962627410889\n",
      "epoch: 285/300, Training loss: 0.015335259959101677, Training accuracy: 0.9794921875\n",
      "Test loss: 0.8525041937828064, Test accuracy: 0.7722222208976746\n",
      "epoch: 286/300, Training loss: 0.056781064718961716, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8802580237388611, Test accuracy: 0.760185182094574\n",
      "epoch: 287/300, Training loss: 0.06243794411420822, Training accuracy: 0.974609375\n",
      "Test loss: 0.8406561017036438, Test accuracy: 0.7768518328666687\n",
      "epoch: 288/300, Training loss: 0.08238263428211212, Training accuracy: 0.9814453125\n",
      "Test loss: 0.7935312986373901, Test accuracy: 0.7870370149612427\n",
      "epoch: 289/300, Training loss: 0.041982799768447876, Training accuracy: 0.9736328125\n",
      "Test loss: 0.8228050470352173, Test accuracy: 0.7833333015441895\n",
      "epoch: 290/300, Training loss: 0.02489309385418892, Training accuracy: 0.98046875\n",
      "Test loss: 0.8593659996986389, Test accuracy: 0.7824074029922485\n",
      "epoch: 291/300, Training loss: 0.06149260699748993, Training accuracy: 0.970703125\n",
      "Test loss: 0.8453108668327332, Test accuracy: 0.7805555462837219\n",
      "epoch: 292/300, Training loss: 0.05570922791957855, Training accuracy: 0.982421875\n",
      "Test loss: 0.8815934658050537, Test accuracy: 0.7694444060325623\n",
      "epoch: 293/300, Training loss: 0.08293069154024124, Training accuracy: 0.9833984375\n",
      "Test loss: 0.8724469542503357, Test accuracy: 0.770370364189148\n",
      "epoch: 294/300, Training loss: 0.08749407529830933, Training accuracy: 0.9755859375\n",
      "Test loss: 0.8769022226333618, Test accuracy: 0.7768518328666687\n",
      "epoch: 295/300, Training loss: 0.07835886627435684, Training accuracy: 0.974609375\n",
      "Test loss: 0.8381355404853821, Test accuracy: 0.7768518328666687\n",
      "epoch: 296/300, Training loss: 0.09558570384979248, Training accuracy: 0.98046875\n",
      "Test loss: 0.8284456729888916, Test accuracy: 0.789814829826355\n",
      "epoch: 297/300, Training loss: 0.03104725107550621, Training accuracy: 0.9716796875\n",
      "Test loss: 0.8347155451774597, Test accuracy: 0.7555555701255798\n",
      "epoch: 298/300, Training loss: 0.12230782210826874, Training accuracy: 0.97265625\n",
      "Test loss: 0.8492111563682556, Test accuracy: 0.770370364189148\n",
      "epoch: 299/300, Training loss: 0.05065964534878731, Training accuracy: 0.9775390625\n",
      "Test loss: 0.8685850501060486, Test accuracy: 0.7879629731178284\n",
      "epoch: 300/300, Training loss: 0.04499232769012451, Training accuracy: 0.9765625\n",
      "Test loss: 0.8383792638778687, Test accuracy: 0.7731481194496155\n",
      "best acc:  tensor(0.7963, device='cuda:0')\n",
      "epoch: 1/300, Training loss: 0.9188180565834045, Training accuracy: 0.662109375\n",
      "Test loss: 1.0161277055740356, Test accuracy: 0.6583333015441895\n",
      "epoch: 2/300, Training loss: 0.6064690351486206, Training accuracy: 0.7041015625\n",
      "Test loss: 0.6827058792114258, Test accuracy: 0.6694444417953491\n",
      "epoch: 3/300, Training loss: 0.5217198133468628, Training accuracy: 0.703125\n",
      "Test loss: 0.5953686833381653, Test accuracy: 0.7240740656852722\n",
      "epoch: 4/300, Training loss: 0.5830798149108887, Training accuracy: 0.720703125\n",
      "Test loss: 0.651339054107666, Test accuracy: 0.6972222328186035\n",
      "epoch: 5/300, Training loss: 0.3820096552371979, Training accuracy: 0.7333984375\n",
      "Test loss: 0.6864390969276428, Test accuracy: 0.6833333373069763\n",
      "epoch: 6/300, Training loss: 0.6634458899497986, Training accuracy: 0.7080078125\n",
      "Test loss: 0.6046332716941833, Test accuracy: 0.720370352268219\n",
      "epoch: 7/300, Training loss: 0.6521608829498291, Training accuracy: 0.7265625\n",
      "Test loss: 0.5864759087562561, Test accuracy: 0.7212963104248047\n",
      "epoch: 8/300, Training loss: 0.5541757941246033, Training accuracy: 0.75390625\n",
      "Test loss: 0.5669588446617126, Test accuracy: 0.7083333134651184\n",
      "epoch: 9/300, Training loss: 0.5824732184410095, Training accuracy: 0.7353515625\n",
      "Test loss: 0.5833855271339417, Test accuracy: 0.7222222089767456\n",
      "epoch: 10/300, Training loss: 0.4559469521045685, Training accuracy: 0.744140625\n",
      "Test loss: 0.5793610215187073, Test accuracy: 0.7268518209457397\n",
      "epoch: 11/300, Training loss: 0.5560258626937866, Training accuracy: 0.7626953125\n",
      "Test loss: 0.6062376499176025, Test accuracy: 0.7138888835906982\n",
      "epoch: 12/300, Training loss: 0.5139526128768921, Training accuracy: 0.751953125\n",
      "Test loss: 0.5980455279350281, Test accuracy: 0.7092592716217041\n",
      "epoch: 13/300, Training loss: 0.6207051277160645, Training accuracy: 0.7490234375\n",
      "Test loss: 0.606239378452301, Test accuracy: 0.7166666388511658\n",
      "epoch: 14/300, Training loss: 0.3790064752101898, Training accuracy: 0.744140625\n",
      "Test loss: 0.5797038674354553, Test accuracy: 0.7092592716217041\n",
      "epoch: 15/300, Training loss: 0.5284121632575989, Training accuracy: 0.751953125\n",
      "Test loss: 0.5979251861572266, Test accuracy: 0.7120370268821716\n",
      "epoch: 16/300, Training loss: 0.5712829232215881, Training accuracy: 0.748046875\n",
      "Test loss: 0.5969832539558411, Test accuracy: 0.7240740656852722\n",
      "epoch: 17/300, Training loss: 0.44401541352272034, Training accuracy: 0.771484375\n",
      "Test loss: 0.5785054564476013, Test accuracy: 0.730555534362793\n",
      "epoch: 18/300, Training loss: 0.5181831121444702, Training accuracy: 0.75\n",
      "Test loss: 0.6001027226448059, Test accuracy: 0.7249999642372131\n",
      "epoch: 19/300, Training loss: 0.4964621365070343, Training accuracy: 0.779296875\n",
      "Test loss: 0.5873205065727234, Test accuracy: 0.7129629254341125\n",
      "epoch: 20/300, Training loss: 0.424490749835968, Training accuracy: 0.7783203125\n",
      "Test loss: 0.5687762498855591, Test accuracy: 0.7342592477798462\n",
      "epoch: 21/300, Training loss: 0.5667546987533569, Training accuracy: 0.7685546875\n",
      "Test loss: 0.5754535794258118, Test accuracy: 0.7416666746139526\n",
      "epoch: 22/300, Training loss: 0.48681777715682983, Training accuracy: 0.794921875\n",
      "Test loss: 0.5891299843788147, Test accuracy: 0.729629635810852\n",
      "epoch: 23/300, Training loss: 0.38616320490837097, Training accuracy: 0.80078125\n",
      "Test loss: 0.5825692415237427, Test accuracy: 0.7324073910713196\n",
      "epoch: 24/300, Training loss: 0.5339969396591187, Training accuracy: 0.791015625\n",
      "Test loss: 0.5975088477134705, Test accuracy: 0.7333333492279053\n",
      "epoch: 25/300, Training loss: 0.5345461964607239, Training accuracy: 0.78125\n",
      "Test loss: 0.6054041981697083, Test accuracy: 0.730555534362793\n",
      "epoch: 26/300, Training loss: 0.4257657527923584, Training accuracy: 0.802734375\n",
      "Test loss: 0.5841351747512817, Test accuracy: 0.7175925970077515\n",
      "epoch: 27/300, Training loss: 0.43913307785987854, Training accuracy: 0.8046875\n",
      "Test loss: 0.5879953503608704, Test accuracy: 0.7314814925193787\n",
      "epoch: 28/300, Training loss: 0.5041826367378235, Training accuracy: 0.79296875\n",
      "Test loss: 0.5912567377090454, Test accuracy: 0.7388888597488403\n",
      "epoch: 29/300, Training loss: 0.3731784224510193, Training accuracy: 0.7998046875\n",
      "Test loss: 0.5503785014152527, Test accuracy: 0.7527777552604675\n",
      "epoch: 30/300, Training loss: 0.36085012555122375, Training accuracy: 0.796875\n",
      "Test loss: 0.5662282109260559, Test accuracy: 0.7444444298744202\n",
      "epoch: 31/300, Training loss: 0.4528127908706665, Training accuracy: 0.796875\n",
      "Test loss: 0.5891615152359009, Test accuracy: 0.7444444298744202\n",
      "epoch: 32/300, Training loss: 0.5048103928565979, Training accuracy: 0.810546875\n",
      "Test loss: 0.5377424955368042, Test accuracy: 0.7768518328666687\n",
      "epoch: 33/300, Training loss: 0.2929604947566986, Training accuracy: 0.8291015625\n",
      "Test loss: 0.5361881852149963, Test accuracy: 0.770370364189148\n",
      "epoch: 34/300, Training loss: 0.33927616477012634, Training accuracy: 0.841796875\n",
      "Test loss: 0.546061098575592, Test accuracy: 0.7592592239379883\n",
      "epoch: 35/300, Training loss: 0.2876347005367279, Training accuracy: 0.8291015625\n",
      "Test loss: 0.5241070985794067, Test accuracy: 0.7805555462837219\n",
      "epoch: 36/300, Training loss: 0.4895687699317932, Training accuracy: 0.8408203125\n",
      "Test loss: 0.537826657295227, Test accuracy: 0.760185182094574\n",
      "epoch: 37/300, Training loss: 0.3873063325881958, Training accuracy: 0.8466796875\n",
      "Test loss: 0.5161416530609131, Test accuracy: 0.7768518328666687\n",
      "epoch: 38/300, Training loss: 0.39105087518692017, Training accuracy: 0.84765625\n",
      "Test loss: 0.5236285328865051, Test accuracy: 0.7777777910232544\n",
      "epoch: 39/300, Training loss: 0.3388260006904602, Training accuracy: 0.8525390625\n",
      "Test loss: 0.5104120969772339, Test accuracy: 0.8092592358589172\n",
      "epoch: 40/300, Training loss: 0.502128005027771, Training accuracy: 0.8525390625\n",
      "Test loss: 0.5182667374610901, Test accuracy: 0.8046296238899231\n",
      "epoch: 41/300, Training loss: 0.611903965473175, Training accuracy: 0.857421875\n",
      "Test loss: 0.5489879846572876, Test accuracy: 0.7842592597007751\n",
      "epoch: 42/300, Training loss: 0.3451598286628723, Training accuracy: 0.8486328125\n",
      "Test loss: 0.5928483605384827, Test accuracy: 0.7462962865829468\n",
      "epoch: 43/300, Training loss: 0.40485599637031555, Training accuracy: 0.845703125\n",
      "Test loss: 0.5386366248130798, Test accuracy: 0.800000011920929\n",
      "epoch: 44/300, Training loss: 0.34234198927879333, Training accuracy: 0.8505859375\n",
      "Test loss: 0.5356420278549194, Test accuracy: 0.7675925493240356\n",
      "epoch: 45/300, Training loss: 0.28471866250038147, Training accuracy: 0.8564453125\n",
      "Test loss: 0.5027385950088501, Test accuracy: 0.8120370507240295\n",
      "epoch: 46/300, Training loss: 0.2857407033443451, Training accuracy: 0.8525390625\n",
      "Test loss: 0.557163417339325, Test accuracy: 0.800000011920929\n",
      "epoch: 47/300, Training loss: 0.3523517847061157, Training accuracy: 0.857421875\n",
      "Test loss: 0.500562310218811, Test accuracy: 0.7944444417953491\n",
      "epoch: 48/300, Training loss: 0.3469216227531433, Training accuracy: 0.8671875\n",
      "Test loss: 0.5563926696777344, Test accuracy: 0.7981481552124023\n",
      "epoch: 49/300, Training loss: 0.35849472880363464, Training accuracy: 0.873046875\n",
      "Test loss: 0.528889536857605, Test accuracy: 0.7981481552124023\n",
      "epoch: 50/300, Training loss: 0.2823643982410431, Training accuracy: 0.8525390625\n",
      "Test loss: 0.5271353721618652, Test accuracy: 0.7962962985038757\n",
      "epoch: 51/300, Training loss: 0.3627359867095947, Training accuracy: 0.853515625\n",
      "Test loss: 0.5315053462982178, Test accuracy: 0.79537034034729\n",
      "epoch: 52/300, Training loss: 0.36240702867507935, Training accuracy: 0.8818359375\n",
      "Test loss: 0.5121375918388367, Test accuracy: 0.8027777671813965\n",
      "epoch: 53/300, Training loss: 0.3063335418701172, Training accuracy: 0.8701171875\n",
      "Test loss: 0.547138512134552, Test accuracy: 0.8009259104728699\n",
      "epoch: 54/300, Training loss: 0.3773021101951599, Training accuracy: 0.873046875\n",
      "Test loss: 0.5039727687835693, Test accuracy: 0.8046296238899231\n",
      "epoch: 55/300, Training loss: 0.2937813401222229, Training accuracy: 0.853515625\n",
      "Test loss: 0.5353946685791016, Test accuracy: 0.7796295881271362\n",
      "epoch: 56/300, Training loss: 0.44653624296188354, Training accuracy: 0.8740234375\n",
      "Test loss: 0.535247266292572, Test accuracy: 0.7888888716697693\n",
      "epoch: 57/300, Training loss: 0.2175462394952774, Training accuracy: 0.873046875\n",
      "Test loss: 0.5093284845352173, Test accuracy: 0.815740704536438\n",
      "epoch: 58/300, Training loss: 0.2930547297000885, Training accuracy: 0.876953125\n",
      "Test loss: 0.554259717464447, Test accuracy: 0.7962962985038757\n",
      "epoch: 59/300, Training loss: 0.31574761867523193, Training accuracy: 0.875\n",
      "Test loss: 0.5427344441413879, Test accuracy: 0.7981481552124023\n",
      "epoch: 60/300, Training loss: 0.26566165685653687, Training accuracy: 0.88671875\n",
      "Test loss: 0.5684137940406799, Test accuracy: 0.7981481552124023\n",
      "epoch: 61/300, Training loss: 0.17248070240020752, Training accuracy: 0.8935546875\n",
      "Test loss: 0.5390987992286682, Test accuracy: 0.7962962985038757\n",
      "epoch: 62/300, Training loss: 0.23809456825256348, Training accuracy: 0.8896484375\n",
      "Test loss: 0.5043171048164368, Test accuracy: 0.8175925612449646\n",
      "epoch: 63/300, Training loss: 0.2453741580247879, Training accuracy: 0.8896484375\n",
      "Test loss: 0.5103262662887573, Test accuracy: 0.8083333373069763\n",
      "epoch: 64/300, Training loss: 0.26121267676353455, Training accuracy: 0.8837890625\n",
      "Test loss: 0.5260341167449951, Test accuracy: 0.8120370507240295\n",
      "epoch: 65/300, Training loss: 0.27331623435020447, Training accuracy: 0.8837890625\n",
      "Test loss: 0.5540273785591125, Test accuracy: 0.8027777671813965\n",
      "epoch: 66/300, Training loss: 0.11017481237649918, Training accuracy: 0.9111328125\n",
      "Test loss: 0.5335768461227417, Test accuracy: 0.8092592358589172\n",
      "epoch: 67/300, Training loss: 0.24338188767433167, Training accuracy: 0.9052734375\n",
      "Test loss: 0.5315923690795898, Test accuracy: 0.8148148059844971\n",
      "epoch: 68/300, Training loss: 0.2665014863014221, Training accuracy: 0.8916015625\n",
      "Test loss: 0.5555341243743896, Test accuracy: 0.7981481552124023\n",
      "epoch: 69/300, Training loss: 0.2535818815231323, Training accuracy: 0.9033203125\n",
      "Test loss: 0.5425058603286743, Test accuracy: 0.8037036657333374\n",
      "epoch: 70/300, Training loss: 0.26087868213653564, Training accuracy: 0.9033203125\n",
      "Test loss: 0.5745745301246643, Test accuracy: 0.7962962985038757\n",
      "epoch: 71/300, Training loss: 0.2010578215122223, Training accuracy: 0.8955078125\n",
      "Test loss: 0.538970947265625, Test accuracy: 0.8046296238899231\n",
      "epoch: 72/300, Training loss: 0.15253570675849915, Training accuracy: 0.9013671875\n",
      "Test loss: 0.5634653568267822, Test accuracy: 0.8027777671813965\n",
      "epoch: 73/300, Training loss: 0.20731329917907715, Training accuracy: 0.896484375\n",
      "Test loss: 0.5298818945884705, Test accuracy: 0.8175925612449646\n",
      "epoch: 74/300, Training loss: 0.31365370750427246, Training accuracy: 0.912109375\n",
      "Test loss: 0.5740978121757507, Test accuracy: 0.8101851940155029\n",
      "epoch: 75/300, Training loss: 0.33808642625808716, Training accuracy: 0.8935546875\n",
      "Test loss: 0.5562921762466431, Test accuracy: 0.800000011920929\n",
      "epoch: 76/300, Training loss: 0.13962864875793457, Training accuracy: 0.89453125\n",
      "Test loss: 0.577096164226532, Test accuracy: 0.805555522441864\n",
      "epoch: 77/300, Training loss: 0.20007118582725525, Training accuracy: 0.8984375\n",
      "Test loss: 0.5811759233474731, Test accuracy: 0.7870370149612427\n",
      "epoch: 78/300, Training loss: 0.20348693430423737, Training accuracy: 0.921875\n",
      "Test loss: 0.562149703502655, Test accuracy: 0.8129629492759705\n",
      "epoch: 79/300, Training loss: 0.20085035264492035, Training accuracy: 0.9033203125\n",
      "Test loss: 0.5424643754959106, Test accuracy: 0.8120370507240295\n",
      "epoch: 80/300, Training loss: 0.2519969046115875, Training accuracy: 0.908203125\n",
      "Test loss: 0.5787577629089355, Test accuracy: 0.8111110925674438\n",
      "epoch: 81/300, Training loss: 0.3883852958679199, Training accuracy: 0.9091796875\n",
      "Test loss: 0.5959537625312805, Test accuracy: 0.8064814805984497\n",
      "epoch: 82/300, Training loss: 0.22938935458660126, Training accuracy: 0.9033203125\n",
      "Test loss: 0.5747082829475403, Test accuracy: 0.7972221970558167\n",
      "epoch: 83/300, Training loss: 0.2542473375797272, Training accuracy: 0.91015625\n",
      "Test loss: 0.5720244646072388, Test accuracy: 0.8064814805984497\n",
      "epoch: 84/300, Training loss: 0.17014385759830475, Training accuracy: 0.912109375\n",
      "Test loss: 0.5881490707397461, Test accuracy: 0.8092592358589172\n",
      "epoch: 85/300, Training loss: 0.22021758556365967, Training accuracy: 0.921875\n",
      "Test loss: 0.561629593372345, Test accuracy: 0.8027777671813965\n",
      "epoch: 86/300, Training loss: 0.19618543982505798, Training accuracy: 0.919921875\n",
      "Test loss: 0.6212537288665771, Test accuracy: 0.8027777671813965\n",
      "epoch: 87/300, Training loss: 0.1627773940563202, Training accuracy: 0.9052734375\n",
      "Test loss: 0.5842990875244141, Test accuracy: 0.8064814805984497\n",
      "epoch: 88/300, Training loss: 0.24827256798744202, Training accuracy: 0.9052734375\n",
      "Test loss: 0.5522521138191223, Test accuracy: 0.8037036657333374\n",
      "epoch: 89/300, Training loss: 0.21973125636577606, Training accuracy: 0.896484375\n",
      "Test loss: 0.6062725782394409, Test accuracy: 0.8046296238899231\n",
      "epoch: 90/300, Training loss: 0.24743545055389404, Training accuracy: 0.9150390625\n",
      "Test loss: 0.6335195302963257, Test accuracy: 0.7962962985038757\n",
      "epoch: 91/300, Training loss: 0.21379168331623077, Training accuracy: 0.9267578125\n",
      "Test loss: 0.5693978071212769, Test accuracy: 0.8046296238899231\n",
      "epoch: 92/300, Training loss: 0.20945344865322113, Training accuracy: 0.9140625\n",
      "Test loss: 0.5940078496932983, Test accuracy: 0.8138888478279114\n",
      "epoch: 93/300, Training loss: 0.11176745593547821, Training accuracy: 0.9189453125\n",
      "Test loss: 0.5705379843711853, Test accuracy: 0.8129629492759705\n",
      "epoch: 94/300, Training loss: 0.19029614329338074, Training accuracy: 0.9169921875\n",
      "Test loss: 0.6175708174705505, Test accuracy: 0.8092592358589172\n",
      "epoch: 95/300, Training loss: 0.13330446183681488, Training accuracy: 0.908203125\n",
      "Test loss: 0.612089216709137, Test accuracy: 0.7972221970558167\n",
      "epoch: 96/300, Training loss: 0.21737141907215118, Training accuracy: 0.9228515625\n",
      "Test loss: 0.5663009881973267, Test accuracy: 0.8138888478279114\n",
      "epoch: 97/300, Training loss: 0.16506816446781158, Training accuracy: 0.9169921875\n",
      "Test loss: 0.6198546290397644, Test accuracy: 0.7944444417953491\n",
      "epoch: 98/300, Training loss: 0.24317006766796112, Training accuracy: 0.923828125\n",
      "Test loss: 0.626120924949646, Test accuracy: 0.8074073791503906\n",
      "epoch: 99/300, Training loss: 0.1269988715648651, Training accuracy: 0.919921875\n",
      "Test loss: 0.580619215965271, Test accuracy: 0.8175925612449646\n",
      "epoch: 100/300, Training loss: 0.16460642218589783, Training accuracy: 0.9228515625\n",
      "Test loss: 0.6358566880226135, Test accuracy: 0.8009259104728699\n",
      "epoch: 101/300, Training loss: 0.15625092387199402, Training accuracy: 0.921875\n",
      "Test loss: 0.5932735800743103, Test accuracy: 0.8212962746620178\n",
      "epoch: 102/300, Training loss: 0.12440581619739532, Training accuracy: 0.9287109375\n",
      "Test loss: 0.5966411232948303, Test accuracy: 0.7925925850868225\n",
      "epoch: 103/300, Training loss: 0.16891643404960632, Training accuracy: 0.92578125\n",
      "Test loss: 0.6291692852973938, Test accuracy: 0.805555522441864\n",
      "epoch: 104/300, Training loss: 0.14131596684455872, Training accuracy: 0.9267578125\n",
      "Test loss: 0.6070537567138672, Test accuracy: 0.8120370507240295\n",
      "epoch: 105/300, Training loss: 0.18601976335048676, Training accuracy: 0.923828125\n",
      "Test loss: 0.6141844987869263, Test accuracy: 0.8018518090248108\n",
      "epoch: 106/300, Training loss: 0.17118845880031586, Training accuracy: 0.9345703125\n",
      "Test loss: 0.6314473748207092, Test accuracy: 0.7944444417953491\n",
      "epoch: 107/300, Training loss: 0.09444289654493332, Training accuracy: 0.927734375\n",
      "Test loss: 0.6016040444374084, Test accuracy: 0.8120370507240295\n",
      "epoch: 108/300, Training loss: 0.16659273207187653, Training accuracy: 0.9296875\n",
      "Test loss: 0.6471767425537109, Test accuracy: 0.800000011920929\n",
      "epoch: 109/300, Training loss: 0.2359854131937027, Training accuracy: 0.9208984375\n",
      "Test loss: 0.6447290778160095, Test accuracy: 0.7972221970558167\n",
      "epoch: 110/300, Training loss: 0.17947375774383545, Training accuracy: 0.9208984375\n",
      "Test loss: 0.6469513177871704, Test accuracy: 0.8037036657333374\n",
      "epoch: 111/300, Training loss: 0.18484334647655487, Training accuracy: 0.9228515625\n",
      "Test loss: 0.6300234794616699, Test accuracy: 0.8027777671813965\n",
      "epoch: 112/300, Training loss: 0.13970936834812164, Training accuracy: 0.919921875\n",
      "Test loss: 0.6392800807952881, Test accuracy: 0.7990740537643433\n",
      "epoch: 113/300, Training loss: 0.12966912984848022, Training accuracy: 0.9287109375\n",
      "Test loss: 0.6658353805541992, Test accuracy: 0.8018518090248108\n",
      "epoch: 114/300, Training loss: 0.1256517469882965, Training accuracy: 0.935546875\n",
      "Test loss: 0.6088769435882568, Test accuracy: 0.8101851940155029\n",
      "epoch: 115/300, Training loss: 0.27129441499710083, Training accuracy: 0.919921875\n",
      "Test loss: 0.5915758609771729, Test accuracy: 0.8037036657333374\n",
      "epoch: 116/300, Training loss: 0.20697669684886932, Training accuracy: 0.9189453125\n",
      "Test loss: 0.6338481307029724, Test accuracy: 0.8120370507240295\n",
      "epoch: 117/300, Training loss: 0.16266031563282013, Training accuracy: 0.93359375\n",
      "Test loss: 0.6233319044113159, Test accuracy: 0.7990740537643433\n",
      "epoch: 118/300, Training loss: 0.2782630920410156, Training accuracy: 0.9208984375\n",
      "Test loss: 0.6222811937332153, Test accuracy: 0.8138888478279114\n",
      "epoch: 119/300, Training loss: 0.15086904168128967, Training accuracy: 0.9287109375\n",
      "Test loss: 0.6398442983627319, Test accuracy: 0.79537034034729\n",
      "epoch: 120/300, Training loss: 0.21725806593894958, Training accuracy: 0.9189453125\n",
      "Test loss: 0.6257085204124451, Test accuracy: 0.7990740537643433\n",
      "epoch: 121/300, Training loss: 0.22919541597366333, Training accuracy: 0.931640625\n",
      "Test loss: 0.6219691634178162, Test accuracy: 0.8175925612449646\n",
      "epoch: 122/300, Training loss: 0.1545088291168213, Training accuracy: 0.93359375\n",
      "Test loss: 0.614569902420044, Test accuracy: 0.800000011920929\n",
      "epoch: 123/300, Training loss: 0.1154717355966568, Training accuracy: 0.9267578125\n",
      "Test loss: 0.6347105503082275, Test accuracy: 0.7935184836387634\n",
      "epoch: 124/300, Training loss: 0.25146010518074036, Training accuracy: 0.927734375\n",
      "Test loss: 0.6792086362838745, Test accuracy: 0.7981481552124023\n",
      "epoch: 125/300, Training loss: 0.19489429891109467, Training accuracy: 0.9287109375\n",
      "Test loss: 0.6433631777763367, Test accuracy: 0.8018518090248108\n",
      "epoch: 126/300, Training loss: 0.2022269070148468, Training accuracy: 0.9345703125\n",
      "Test loss: 0.6270729303359985, Test accuracy: 0.8101851940155029\n",
      "epoch: 127/300, Training loss: 0.1684579849243164, Training accuracy: 0.9228515625\n",
      "Test loss: 0.628067672252655, Test accuracy: 0.800000011920929\n",
      "epoch: 128/300, Training loss: 0.14638973772525787, Training accuracy: 0.9375\n",
      "Test loss: 0.6261415481567383, Test accuracy: 0.8148148059844971\n",
      "epoch: 129/300, Training loss: 0.13869352638721466, Training accuracy: 0.9296875\n",
      "Test loss: 0.637410581111908, Test accuracy: 0.800000011920929\n",
      "epoch: 130/300, Training loss: 0.22519215941429138, Training accuracy: 0.93359375\n",
      "Test loss: 0.6246846318244934, Test accuracy: 0.7870370149612427\n",
      "epoch: 131/300, Training loss: 0.16470855474472046, Training accuracy: 0.9267578125\n",
      "Test loss: 0.6610368490219116, Test accuracy: 0.8027777671813965\n",
      "epoch: 132/300, Training loss: 0.18721342086791992, Training accuracy: 0.93359375\n",
      "Test loss: 0.6301534175872803, Test accuracy: 0.8027777671813965\n",
      "epoch: 133/300, Training loss: 0.12073347717523575, Training accuracy: 0.923828125\n",
      "Test loss: 0.6574420928955078, Test accuracy: 0.789814829826355\n",
      "epoch: 134/300, Training loss: 0.3122371435165405, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6498192548751831, Test accuracy: 0.7981481552124023\n",
      "epoch: 135/300, Training loss: 0.11332939565181732, Training accuracy: 0.935546875\n",
      "Test loss: 0.646377444267273, Test accuracy: 0.8027777671813965\n",
      "epoch: 136/300, Training loss: 0.18089091777801514, Training accuracy: 0.93359375\n",
      "Test loss: 0.6140573024749756, Test accuracy: 0.8148148059844971\n",
      "epoch: 137/300, Training loss: 0.13546454906463623, Training accuracy: 0.94140625\n",
      "Test loss: 0.6394617557525635, Test accuracy: 0.7990740537643433\n",
      "epoch: 138/300, Training loss: 0.11624560505151749, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6421568989753723, Test accuracy: 0.7935184836387634\n",
      "epoch: 139/300, Training loss: 0.13389217853546143, Training accuracy: 0.939453125\n",
      "Test loss: 0.6294354200363159, Test accuracy: 0.8046296238899231\n",
      "epoch: 140/300, Training loss: 0.1750507950782776, Training accuracy: 0.9267578125\n",
      "Test loss: 0.645897626876831, Test accuracy: 0.8194444179534912\n",
      "epoch: 141/300, Training loss: 0.13776500523090363, Training accuracy: 0.9326171875\n",
      "Test loss: 0.656408429145813, Test accuracy: 0.800000011920929\n",
      "epoch: 142/300, Training loss: 0.18124069273471832, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6074065566062927, Test accuracy: 0.8111110925674438\n",
      "epoch: 143/300, Training loss: 0.13417184352874756, Training accuracy: 0.927734375\n",
      "Test loss: 0.6234588623046875, Test accuracy: 0.7916666269302368\n",
      "epoch: 144/300, Training loss: 0.1841759830713272, Training accuracy: 0.9248046875\n",
      "Test loss: 0.6550919413566589, Test accuracy: 0.7925925850868225\n",
      "epoch: 145/300, Training loss: 0.23560260236263275, Training accuracy: 0.935546875\n",
      "Test loss: 0.6503987312316895, Test accuracy: 0.7962962985038757\n",
      "epoch: 146/300, Training loss: 0.1394665241241455, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6139647960662842, Test accuracy: 0.7962962985038757\n",
      "epoch: 147/300, Training loss: 0.1821906715631485, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6423320174217224, Test accuracy: 0.8101851940155029\n",
      "epoch: 148/300, Training loss: 0.15209081768989563, Training accuracy: 0.927734375\n",
      "Test loss: 0.6406947374343872, Test accuracy: 0.8120370507240295\n",
      "epoch: 149/300, Training loss: 0.12487226724624634, Training accuracy: 0.931640625\n",
      "Test loss: 0.6300029754638672, Test accuracy: 0.8148148059844971\n",
      "epoch: 150/300, Training loss: 0.12412906438112259, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6401198506355286, Test accuracy: 0.8101851940155029\n",
      "epoch: 151/300, Training loss: 0.08364046365022659, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6157189607620239, Test accuracy: 0.8222222328186035\n",
      "epoch: 152/300, Training loss: 0.19470573961734772, Training accuracy: 0.9375\n",
      "Test loss: 0.6414778232574463, Test accuracy: 0.815740704536438\n",
      "epoch: 153/300, Training loss: 0.06731552630662918, Training accuracy: 0.9248046875\n",
      "Test loss: 0.6745679974555969, Test accuracy: 0.8009259104728699\n",
      "epoch: 154/300, Training loss: 0.07144171744585037, Training accuracy: 0.9287109375\n",
      "Test loss: 0.6487036347389221, Test accuracy: 0.8037036657333374\n",
      "epoch: 155/300, Training loss: 0.17483627796173096, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6346868276596069, Test accuracy: 0.8064814805984497\n",
      "epoch: 156/300, Training loss: 0.13514335453510284, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6819732189178467, Test accuracy: 0.7944444417953491\n",
      "epoch: 157/300, Training loss: 0.14078405499458313, Training accuracy: 0.91796875\n",
      "Test loss: 0.6366502642631531, Test accuracy: 0.8129629492759705\n",
      "epoch: 158/300, Training loss: 0.19254544377326965, Training accuracy: 0.9296875\n",
      "Test loss: 0.6362212300300598, Test accuracy: 0.7962962985038757\n",
      "epoch: 159/300, Training loss: 0.1711570769548416, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6565086245536804, Test accuracy: 0.805555522441864\n",
      "epoch: 160/300, Training loss: 0.18551184237003326, Training accuracy: 0.9404296875\n",
      "Test loss: 0.6039369702339172, Test accuracy: 0.8175925612449646\n",
      "epoch: 161/300, Training loss: 0.15681029856204987, Training accuracy: 0.931640625\n",
      "Test loss: 0.6332041025161743, Test accuracy: 0.8074073791503906\n",
      "epoch: 162/300, Training loss: 0.11777470260858536, Training accuracy: 0.9462890625\n",
      "Test loss: 0.6474485397338867, Test accuracy: 0.7990740537643433\n",
      "epoch: 163/300, Training loss: 0.21319912374019623, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6477262377738953, Test accuracy: 0.8064814805984497\n",
      "epoch: 164/300, Training loss: 0.23091913759708405, Training accuracy: 0.9345703125\n",
      "Test loss: 0.67522132396698, Test accuracy: 0.8037036657333374\n",
      "epoch: 165/300, Training loss: 0.12280344218015671, Training accuracy: 0.9345703125\n",
      "Test loss: 0.6495897769927979, Test accuracy: 0.8009259104728699\n",
      "epoch: 166/300, Training loss: 0.1799267679452896, Training accuracy: 0.93359375\n",
      "Test loss: 0.6708458662033081, Test accuracy: 0.7990740537643433\n",
      "epoch: 167/300, Training loss: 0.1923282891511917, Training accuracy: 0.9296875\n",
      "Test loss: 0.6505909562110901, Test accuracy: 0.7990740537643433\n",
      "epoch: 168/300, Training loss: 0.19890320301055908, Training accuracy: 0.9443359375\n",
      "Test loss: 0.6720185279846191, Test accuracy: 0.8074073791503906\n",
      "epoch: 169/300, Training loss: 0.10825446993112564, Training accuracy: 0.935546875\n",
      "Test loss: 0.6833886504173279, Test accuracy: 0.7990740537643433\n",
      "epoch: 170/300, Training loss: 0.15566858649253845, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6604428291320801, Test accuracy: 0.7981481552124023\n",
      "epoch: 171/300, Training loss: 0.14672666788101196, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6318760514259338, Test accuracy: 0.7990740537643433\n",
      "epoch: 172/300, Training loss: 0.10592909902334213, Training accuracy: 0.943359375\n",
      "Test loss: 0.6542284488677979, Test accuracy: 0.8009259104728699\n",
      "epoch: 173/300, Training loss: 0.16514436900615692, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6466346979141235, Test accuracy: 0.8138888478279114\n",
      "epoch: 174/300, Training loss: 0.16753216087818146, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6663866639137268, Test accuracy: 0.8148148059844971\n",
      "epoch: 175/300, Training loss: 0.15464745461940765, Training accuracy: 0.9404296875\n",
      "Test loss: 0.670592725276947, Test accuracy: 0.8037036657333374\n",
      "epoch: 176/300, Training loss: 0.1873840093612671, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6380212306976318, Test accuracy: 0.805555522441864\n",
      "epoch: 177/300, Training loss: 0.09109597653150558, Training accuracy: 0.943359375\n",
      "Test loss: 0.6633890271186829, Test accuracy: 0.805555522441864\n",
      "epoch: 178/300, Training loss: 0.11175911873579025, Training accuracy: 0.935546875\n",
      "Test loss: 0.6305699348449707, Test accuracy: 0.8129629492759705\n",
      "epoch: 179/300, Training loss: 0.12065421044826508, Training accuracy: 0.93359375\n",
      "Test loss: 0.6715823411941528, Test accuracy: 0.7907407283782959\n",
      "epoch: 180/300, Training loss: 0.16498485207557678, Training accuracy: 0.931640625\n",
      "Test loss: 0.6370563507080078, Test accuracy: 0.805555522441864\n",
      "epoch: 181/300, Training loss: 0.21610529720783234, Training accuracy: 0.94140625\n",
      "Test loss: 0.6628351807594299, Test accuracy: 0.805555522441864\n",
      "epoch: 182/300, Training loss: 0.14242029190063477, Training accuracy: 0.9462890625\n",
      "Test loss: 0.6545370817184448, Test accuracy: 0.8046296238899231\n",
      "epoch: 183/300, Training loss: 0.2407078742980957, Training accuracy: 0.943359375\n",
      "Test loss: 0.6350736021995544, Test accuracy: 0.8166666626930237\n",
      "epoch: 184/300, Training loss: 0.09997071325778961, Training accuracy: 0.9404296875\n",
      "Test loss: 0.640602171421051, Test accuracy: 0.8092592358589172\n",
      "epoch: 185/300, Training loss: 0.1472494900226593, Training accuracy: 0.939453125\n",
      "Test loss: 0.651719331741333, Test accuracy: 0.8027777671813965\n",
      "epoch: 186/300, Training loss: 0.12552405893802643, Training accuracy: 0.9404296875\n",
      "Test loss: 0.6352195739746094, Test accuracy: 0.8037036657333374\n",
      "epoch: 187/300, Training loss: 0.22399048507213593, Training accuracy: 0.939453125\n",
      "Test loss: 0.6629262566566467, Test accuracy: 0.79537034034729\n",
      "epoch: 188/300, Training loss: 0.11497963964939117, Training accuracy: 0.935546875\n",
      "Test loss: 0.651626706123352, Test accuracy: 0.8046296238899231\n",
      "epoch: 189/300, Training loss: 0.0643225908279419, Training accuracy: 0.9453125\n",
      "Test loss: 0.6657527685165405, Test accuracy: 0.805555522441864\n",
      "epoch: 190/300, Training loss: 0.07923318445682526, Training accuracy: 0.9375\n",
      "Test loss: 0.5984376668930054, Test accuracy: 0.805555522441864\n",
      "epoch: 191/300, Training loss: 0.18461747467517853, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6425151824951172, Test accuracy: 0.8027777671813965\n",
      "epoch: 192/300, Training loss: 0.22019317746162415, Training accuracy: 0.94140625\n",
      "Test loss: 0.6833436489105225, Test accuracy: 0.8009259104728699\n",
      "epoch: 193/300, Training loss: 0.10908755660057068, Training accuracy: 0.943359375\n",
      "Test loss: 0.6692013740539551, Test accuracy: 0.7907407283782959\n",
      "epoch: 194/300, Training loss: 0.12464785575866699, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6598194241523743, Test accuracy: 0.8027777671813965\n",
      "epoch: 195/300, Training loss: 0.24780403077602386, Training accuracy: 0.94140625\n",
      "Test loss: 0.6377040147781372, Test accuracy: 0.8129629492759705\n",
      "epoch: 196/300, Training loss: 0.15283800661563873, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6517319083213806, Test accuracy: 0.8138888478279114\n",
      "epoch: 197/300, Training loss: 0.15872035920619965, Training accuracy: 0.9443359375\n",
      "Test loss: 0.6287059783935547, Test accuracy: 0.7990740537643433\n",
      "epoch: 198/300, Training loss: 0.139841690659523, Training accuracy: 0.94140625\n",
      "Test loss: 0.6492474675178528, Test accuracy: 0.79537034034729\n",
      "epoch: 199/300, Training loss: 0.17739200592041016, Training accuracy: 0.9443359375\n",
      "Test loss: 0.6848939657211304, Test accuracy: 0.7981481552124023\n",
      "epoch: 200/300, Training loss: 0.16154342889785767, Training accuracy: 0.95703125\n",
      "Test loss: 0.6398923993110657, Test accuracy: 0.805555522441864\n",
      "epoch: 201/300, Training loss: 0.06939642876386642, Training accuracy: 0.94140625\n",
      "Test loss: 0.6627028584480286, Test accuracy: 0.8037036657333374\n",
      "epoch: 202/300, Training loss: 0.11155342310667038, Training accuracy: 0.94140625\n",
      "Test loss: 0.6573575139045715, Test accuracy: 0.8092592358589172\n",
      "epoch: 203/300, Training loss: 0.12840640544891357, Training accuracy: 0.9375\n",
      "Test loss: 0.6421546936035156, Test accuracy: 0.8175925612449646\n",
      "epoch: 204/300, Training loss: 0.24826925992965698, Training accuracy: 0.931640625\n",
      "Test loss: 0.644480288028717, Test accuracy: 0.7916666269302368\n",
      "epoch: 205/300, Training loss: 0.25111764669418335, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6570169925689697, Test accuracy: 0.79537034034729\n",
      "epoch: 206/300, Training loss: 0.08926091343164444, Training accuracy: 0.943359375\n",
      "Test loss: 0.6691477298736572, Test accuracy: 0.800000011920929\n",
      "epoch: 207/300, Training loss: 0.10327213257551193, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6409975290298462, Test accuracy: 0.7990740537643433\n",
      "epoch: 208/300, Training loss: 0.1311589479446411, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6387672424316406, Test accuracy: 0.8212962746620178\n",
      "epoch: 209/300, Training loss: 0.14259518682956696, Training accuracy: 0.939453125\n",
      "Test loss: 0.616948664188385, Test accuracy: 0.8148148059844971\n",
      "epoch: 210/300, Training loss: 0.09244552254676819, Training accuracy: 0.9501953125\n",
      "Test loss: 0.6721593141555786, Test accuracy: 0.8074073791503906\n",
      "epoch: 211/300, Training loss: 0.11498629301786423, Training accuracy: 0.9345703125\n",
      "Test loss: 0.6343047618865967, Test accuracy: 0.8064814805984497\n",
      "epoch: 212/300, Training loss: 0.1622677445411682, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6336644291877747, Test accuracy: 0.8120370507240295\n",
      "epoch: 213/300, Training loss: 0.1994600147008896, Training accuracy: 0.9287109375\n",
      "Test loss: 0.6492846012115479, Test accuracy: 0.8064814805984497\n",
      "epoch: 214/300, Training loss: 0.08674599975347519, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6167308688163757, Test accuracy: 0.8027777671813965\n",
      "epoch: 215/300, Training loss: 0.2543436288833618, Training accuracy: 0.93359375\n",
      "Test loss: 0.6032768487930298, Test accuracy: 0.8175925612449646\n",
      "epoch: 216/300, Training loss: 0.10502628982067108, Training accuracy: 0.939453125\n",
      "Test loss: 0.6656284928321838, Test accuracy: 0.8027777671813965\n",
      "epoch: 217/300, Training loss: 0.109782375395298, Training accuracy: 0.9453125\n",
      "Test loss: 0.6795904040336609, Test accuracy: 0.800000011920929\n",
      "epoch: 218/300, Training loss: 0.12048029899597168, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6833059787750244, Test accuracy: 0.7962962985038757\n",
      "epoch: 219/300, Training loss: 0.08520787209272385, Training accuracy: 0.9443359375\n",
      "Test loss: 0.646983802318573, Test accuracy: 0.800000011920929\n",
      "epoch: 220/300, Training loss: 0.0897192507982254, Training accuracy: 0.935546875\n",
      "Test loss: 0.6489792466163635, Test accuracy: 0.79537034034729\n",
      "epoch: 221/300, Training loss: 0.1682300865650177, Training accuracy: 0.921875\n",
      "Test loss: 0.6204372644424438, Test accuracy: 0.805555522441864\n",
      "epoch: 222/300, Training loss: 0.1317758411169052, Training accuracy: 0.9345703125\n",
      "Test loss: 0.6518985033035278, Test accuracy: 0.800000011920929\n",
      "epoch: 223/300, Training loss: 0.18034471571445465, Training accuracy: 0.935546875\n",
      "Test loss: 0.6624267101287842, Test accuracy: 0.8111110925674438\n",
      "epoch: 224/300, Training loss: 0.1630363166332245, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6119922399520874, Test accuracy: 0.8129629492759705\n",
      "epoch: 225/300, Training loss: 0.13740387558937073, Training accuracy: 0.9462890625\n",
      "Test loss: 0.6371359825134277, Test accuracy: 0.7981481552124023\n",
      "epoch: 226/300, Training loss: 0.10671962052583694, Training accuracy: 0.93359375\n",
      "Test loss: 0.6727069616317749, Test accuracy: 0.7990740537643433\n",
      "epoch: 227/300, Training loss: 0.18089550733566284, Training accuracy: 0.9267578125\n",
      "Test loss: 0.6575296521186829, Test accuracy: 0.8101851940155029\n",
      "epoch: 228/300, Training loss: 0.10780870914459229, Training accuracy: 0.939453125\n",
      "Test loss: 0.6636219620704651, Test accuracy: 0.8092592358589172\n",
      "epoch: 229/300, Training loss: 0.12943893671035767, Training accuracy: 0.9384765625\n",
      "Test loss: 0.657895028591156, Test accuracy: 0.7935184836387634\n",
      "epoch: 230/300, Training loss: 0.20133619010448456, Training accuracy: 0.9296875\n",
      "Test loss: 0.6338200569152832, Test accuracy: 0.8212962746620178\n",
      "epoch: 231/300, Training loss: 0.12151303142309189, Training accuracy: 0.9306640625\n",
      "Test loss: 0.6758056879043579, Test accuracy: 0.800000011920929\n",
      "epoch: 232/300, Training loss: 0.10898330807685852, Training accuracy: 0.94140625\n",
      "Test loss: 0.6383461952209473, Test accuracy: 0.7962962985038757\n",
      "epoch: 233/300, Training loss: 0.10582375526428223, Training accuracy: 0.9375\n",
      "Test loss: 0.6522338390350342, Test accuracy: 0.8120370507240295\n",
      "epoch: 234/300, Training loss: 0.14988544583320618, Training accuracy: 0.9248046875\n",
      "Test loss: 0.6379848122596741, Test accuracy: 0.8092592358589172\n",
      "epoch: 235/300, Training loss: 0.18126904964447021, Training accuracy: 0.9453125\n",
      "Test loss: 0.6541714072227478, Test accuracy: 0.8064814805984497\n",
      "epoch: 236/300, Training loss: 0.09295887500047684, Training accuracy: 0.93359375\n",
      "Test loss: 0.6517685651779175, Test accuracy: 0.8074073791503906\n",
      "epoch: 237/300, Training loss: 0.17941932380199432, Training accuracy: 0.91015625\n",
      "Test loss: 0.6254993081092834, Test accuracy: 0.805555522441864\n",
      "epoch: 238/300, Training loss: 0.18514785170555115, Training accuracy: 0.939453125\n",
      "Test loss: 0.6468090415000916, Test accuracy: 0.7962962985038757\n",
      "epoch: 239/300, Training loss: 0.17545437812805176, Training accuracy: 0.9375\n",
      "Test loss: 0.6622424721717834, Test accuracy: 0.8129629492759705\n",
      "epoch: 240/300, Training loss: 0.28187593817710876, Training accuracy: 0.939453125\n",
      "Test loss: 0.6385443806648254, Test accuracy: 0.8166666626930237\n",
      "epoch: 241/300, Training loss: 0.0877952054142952, Training accuracy: 0.939453125\n",
      "Test loss: 0.6245275139808655, Test accuracy: 0.815740704536438\n",
      "epoch: 242/300, Training loss: 0.25462016463279724, Training accuracy: 0.935546875\n",
      "Test loss: 0.655235230922699, Test accuracy: 0.8083333373069763\n",
      "epoch: 243/300, Training loss: 0.10274060815572739, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6341906189918518, Test accuracy: 0.8074073791503906\n",
      "epoch: 244/300, Training loss: 0.19356021285057068, Training accuracy: 0.9375\n",
      "Test loss: 0.6397262215614319, Test accuracy: 0.8064814805984497\n",
      "epoch: 245/300, Training loss: 0.2641557455062866, Training accuracy: 0.9375\n",
      "Test loss: 0.6270344257354736, Test accuracy: 0.8046296238899231\n",
      "epoch: 246/300, Training loss: 0.09785597771406174, Training accuracy: 0.9404296875\n",
      "Test loss: 0.6816802620887756, Test accuracy: 0.8027777671813965\n",
      "epoch: 247/300, Training loss: 0.09825693815946579, Training accuracy: 0.93359375\n",
      "Test loss: 0.6805555820465088, Test accuracy: 0.7935184836387634\n",
      "epoch: 248/300, Training loss: 0.19837182760238647, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6384219527244568, Test accuracy: 0.7962962985038757\n",
      "epoch: 249/300, Training loss: 0.18565988540649414, Training accuracy: 0.9345703125\n",
      "Test loss: 0.6458304524421692, Test accuracy: 0.7972221970558167\n",
      "epoch: 250/300, Training loss: 0.20228265225887299, Training accuracy: 0.9404296875\n",
      "Test loss: 0.6522461771965027, Test accuracy: 0.805555522441864\n",
      "epoch: 251/300, Training loss: 0.22256822884082794, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6339462995529175, Test accuracy: 0.805555522441864\n",
      "epoch: 252/300, Training loss: 0.16798917949199677, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6227607727050781, Test accuracy: 0.8101851940155029\n",
      "epoch: 253/300, Training loss: 0.13062548637390137, Training accuracy: 0.9443359375\n",
      "Test loss: 0.6385757327079773, Test accuracy: 0.8018518090248108\n",
      "epoch: 254/300, Training loss: 0.17221441864967346, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6314921975135803, Test accuracy: 0.7925925850868225\n",
      "epoch: 255/300, Training loss: 0.18185874819755554, Training accuracy: 0.91796875\n",
      "Test loss: 0.6539413928985596, Test accuracy: 0.805555522441864\n",
      "epoch: 256/300, Training loss: 0.1572011560201645, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6899189352989197, Test accuracy: 0.7990740537643433\n",
      "epoch: 257/300, Training loss: 0.1602572798728943, Training accuracy: 0.935546875\n",
      "Test loss: 0.6231202483177185, Test accuracy: 0.8129629492759705\n",
      "epoch: 258/300, Training loss: 0.09960228204727173, Training accuracy: 0.927734375\n",
      "Test loss: 0.6157599687576294, Test accuracy: 0.8138888478279114\n",
      "epoch: 259/300, Training loss: 0.21604982018470764, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6486554145812988, Test accuracy: 0.8037036657333374\n",
      "epoch: 260/300, Training loss: 0.1427147090435028, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6285005211830139, Test accuracy: 0.8129629492759705\n",
      "epoch: 261/300, Training loss: 0.20812353491783142, Training accuracy: 0.931640625\n",
      "Test loss: 0.634192168712616, Test accuracy: 0.7990740537643433\n",
      "epoch: 262/300, Training loss: 0.272487610578537, Training accuracy: 0.9326171875\n",
      "Test loss: 0.6785749793052673, Test accuracy: 0.8027777671813965\n",
      "epoch: 263/300, Training loss: 0.09928769618272781, Training accuracy: 0.94140625\n",
      "Test loss: 0.6632349491119385, Test accuracy: 0.8120370507240295\n",
      "epoch: 264/300, Training loss: 0.15423646569252014, Training accuracy: 0.9443359375\n",
      "Test loss: 0.6308004856109619, Test accuracy: 0.8009259104728699\n",
      "epoch: 265/300, Training loss: 0.12005005031824112, Training accuracy: 0.93359375\n",
      "Test loss: 0.6672638654708862, Test accuracy: 0.7935184836387634\n",
      "epoch: 266/300, Training loss: 0.15995396673679352, Training accuracy: 0.9375\n",
      "Test loss: 0.6375661492347717, Test accuracy: 0.815740704536438\n",
      "epoch: 267/300, Training loss: 0.2090151011943817, Training accuracy: 0.931640625\n",
      "Test loss: 0.624877393245697, Test accuracy: 0.8064814805984497\n",
      "epoch: 268/300, Training loss: 0.09162817895412445, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6465017199516296, Test accuracy: 0.8111110925674438\n",
      "epoch: 269/300, Training loss: 0.17672866582870483, Training accuracy: 0.9326171875\n",
      "Test loss: 0.636443555355072, Test accuracy: 0.8009259104728699\n",
      "epoch: 270/300, Training loss: 0.16184426844120026, Training accuracy: 0.9296875\n",
      "Test loss: 0.6514761447906494, Test accuracy: 0.8037036657333374\n",
      "epoch: 271/300, Training loss: 0.17272378504276276, Training accuracy: 0.931640625\n",
      "Test loss: 0.6072738766670227, Test accuracy: 0.8064814805984497\n",
      "epoch: 272/300, Training loss: 0.06794258952140808, Training accuracy: 0.9375\n",
      "Test loss: 0.6588409543037415, Test accuracy: 0.7981481552124023\n",
      "epoch: 273/300, Training loss: 0.06549345701932907, Training accuracy: 0.939453125\n",
      "Test loss: 0.6615344285964966, Test accuracy: 0.7944444417953491\n",
      "epoch: 274/300, Training loss: 0.14458343386650085, Training accuracy: 0.94140625\n",
      "Test loss: 0.7073535919189453, Test accuracy: 0.7907407283782959\n",
      "epoch: 275/300, Training loss: 0.14396704733371735, Training accuracy: 0.9462890625\n",
      "Test loss: 0.6493072509765625, Test accuracy: 0.8148148059844971\n",
      "epoch: 276/300, Training loss: 0.19758404791355133, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6626670360565186, Test accuracy: 0.8009259104728699\n",
      "epoch: 277/300, Training loss: 0.09560161828994751, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6408388614654541, Test accuracy: 0.8027777671813965\n",
      "epoch: 278/300, Training loss: 0.1339946687221527, Training accuracy: 0.9384765625\n",
      "Test loss: 0.673511266708374, Test accuracy: 0.8037036657333374\n",
      "epoch: 279/300, Training loss: 0.14362943172454834, Training accuracy: 0.935546875\n",
      "Test loss: 0.6677185297012329, Test accuracy: 0.8018518090248108\n",
      "epoch: 280/300, Training loss: 0.16239766776561737, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6699891090393066, Test accuracy: 0.8064814805984497\n",
      "epoch: 281/300, Training loss: 0.13948486745357513, Training accuracy: 0.943359375\n",
      "Test loss: 0.6242372989654541, Test accuracy: 0.8092592358589172\n",
      "epoch: 282/300, Training loss: 0.16579020023345947, Training accuracy: 0.939453125\n",
      "Test loss: 0.648343026638031, Test accuracy: 0.8074073791503906\n",
      "epoch: 283/300, Training loss: 0.08576399087905884, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6575329303741455, Test accuracy: 0.8111110925674438\n",
      "epoch: 284/300, Training loss: 0.13795872032642365, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6136807203292847, Test accuracy: 0.8148148059844971\n",
      "epoch: 285/300, Training loss: 0.11613381654024124, Training accuracy: 0.9423828125\n",
      "Test loss: 0.668855607509613, Test accuracy: 0.800000011920929\n",
      "epoch: 286/300, Training loss: 0.10115606337785721, Training accuracy: 0.9443359375\n",
      "Test loss: 0.666037380695343, Test accuracy: 0.789814829826355\n",
      "epoch: 287/300, Training loss: 0.15628378093242645, Training accuracy: 0.939453125\n",
      "Test loss: 0.6148697733879089, Test accuracy: 0.8046296238899231\n",
      "epoch: 288/300, Training loss: 0.15650635957717896, Training accuracy: 0.9267578125\n",
      "Test loss: 0.6881250739097595, Test accuracy: 0.7916666269302368\n",
      "epoch: 289/300, Training loss: 0.12580440938472748, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6319608092308044, Test accuracy: 0.8027777671813965\n",
      "epoch: 290/300, Training loss: 0.19848543405532837, Training accuracy: 0.935546875\n",
      "Test loss: 0.6309510469436646, Test accuracy: 0.8018518090248108\n",
      "epoch: 291/300, Training loss: 0.10944250226020813, Training accuracy: 0.9384765625\n",
      "Test loss: 0.6252536177635193, Test accuracy: 0.805555522441864\n",
      "epoch: 292/300, Training loss: 0.16414880752563477, Training accuracy: 0.939453125\n",
      "Test loss: 0.6526904106140137, Test accuracy: 0.8074073791503906\n",
      "epoch: 293/300, Training loss: 0.1590183973312378, Training accuracy: 0.9423828125\n",
      "Test loss: 0.6559559106826782, Test accuracy: 0.7981481552124023\n",
      "epoch: 294/300, Training loss: 0.12193147838115692, Training accuracy: 0.9404296875\n",
      "Test loss: 0.656886637210846, Test accuracy: 0.8046296238899231\n",
      "epoch: 295/300, Training loss: 0.05920272693037987, Training accuracy: 0.94921875\n",
      "Test loss: 0.6273818612098694, Test accuracy: 0.8009259104728699\n",
      "epoch: 296/300, Training loss: 0.0964571163058281, Training accuracy: 0.93359375\n",
      "Test loss: 0.6499344706535339, Test accuracy: 0.7925925850868225\n",
      "epoch: 297/300, Training loss: 0.09649834781885147, Training accuracy: 0.927734375\n",
      "Test loss: 0.6403436660766602, Test accuracy: 0.8018518090248108\n",
      "epoch: 298/300, Training loss: 0.1314512938261032, Training accuracy: 0.939453125\n",
      "Test loss: 0.6473360657691956, Test accuracy: 0.805555522441864\n",
      "epoch: 299/300, Training loss: 0.18503043055534363, Training accuracy: 0.9365234375\n",
      "Test loss: 0.6316012740135193, Test accuracy: 0.8092592358589172\n",
      "epoch: 300/300, Training loss: 0.15351852774620056, Training accuracy: 0.9404296875\n",
      "Test loss: 0.627643346786499, Test accuracy: 0.8074073791503906\n",
      "best acc:  tensor(0.8222, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EEGnet(\n",
       "  (firstconv): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(1, 51), stride=(1, 1), padding=(0, 25), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (depthwiseConv): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(2, 1), stride=(1, 1), groups=16, bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
       "    (4): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (separableConv): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(1, 15), stride=(1, 1), padding=(0, 7), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
       "    (4): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (classify): Linear(in_features=736, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''nets = [\n",
    "    DeepConvNet(nn.LeakyReLU()).to(device),\n",
    "    DeepConvNet(nn.ReLU()).to(device),\n",
    "    DeepConvNet(nn.ELU()).to(device)\n",
    "]'''\n",
    "nets = [\n",
    "    EEGnet(nn.LeakyReLU()).to(device),\n",
    "    EEGnet(nn.ReLU()).to(device),\n",
    "    EEGnet(nn.ELU()).to(device)\n",
    "]\n",
    "name = [\"LeakyReLU\", \"ReLU\", \"ELU\"]\n",
    "\n",
    "for i, net in enumerate(nets):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    best_test_acc = 0\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = 0.02)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=False, min_lr=0.00001, cooldown=10)\n",
    "\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        total_acc = 0\n",
    "        total_item = 0\n",
    "        for (x, y_true) in train_loader:\n",
    "            y_pred = net(x)\n",
    "            output = loss(y_pred, y_true)\n",
    "            net.zero_grad()\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            _, y_pred = torch.max(y_pred.data, 1)\n",
    "            total_acc += (y_pred == y_true).sum()\n",
    "            total_item += y_pred.shape[0]\n",
    "\n",
    "        accuracy = total_acc / total_item\n",
    "        train_loss.append(output.item())\n",
    "        train_acc.append(accuracy)\n",
    "        print(f\"epoch: {epoch}/{EPOCH}, Training loss: {output.item()}, Training accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = net(test_data)\n",
    "            output = loss(y_pred, test_label)\n",
    "\n",
    "        _, y_pred = torch.max(y_pred.data, 1)\n",
    "        accuracy = (y_pred == test_label).sum()/train_data.shape[0]\n",
    "        test_loss.append(output.item())\n",
    "        test_acc.append(accuracy)\n",
    "        print(f\"Test loss: {output.item()}, Test accuracy: {accuracy}\")\n",
    "        if accuracy > best_test_acc:\n",
    "            best_test_acc = accuracy\n",
    "\n",
    "        scheduler.step(output.item())\n",
    "\n",
    "    plt.plot(train_acc, label=name[i]+\"_train\")\n",
    "    plt.plot(test_acc, label=name[i]+\"_test\")\n",
    "\n",
    "    print(\"best acc: \", best_test_acc)\n",
    "\n",
    "plt.title(\"Activation function comparason(DeepConvNet)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy(%)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c7f78ee7ae4c5c3fcd15e9dc69fa83eee8bad7b80870798e03547f69f238c1a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
